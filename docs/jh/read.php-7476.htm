<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-cn">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="UTF-8" />
<meta content="all" name="robots" />
<meta name="author" content="流浪的龙－个人知识管理" />
<meta name="description" content="" />
<meta name="keywords" content="bo-blog" />
<xbasehref="http://i.renjihe.com/blog/" />
<link rel="alternate" title="流浪的龙－个人知识管理" href="feed.php" type="application/rss+xml" />
<link rel="stylesheet" rev="stylesheet" href="styles.css" tppabs="http://i.renjihe.com/blog/template/default/styles.css" type="text/css" media="all" />

<title>大牛的《深度学习》笔记，Deep Learning速成教程 - 流浪的龙－个人知识管理 - </title>
<script type="text/javascript" src="common.js-jsver=2.1.1.3626.3" tppabs="http://i.renjihe.com/blog/images/js/common.js?jsver=2.1.1.3626.3"></script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://i.renjihe.com/blog/inc/rsd.php" />
<script type="text/javascript" src="jslang.js-jsver=2.1.1.3626.3" tppabs="http://i.renjihe.com/blog/lang/zh-cn/jslang.js?jsver=2.1.1.3626.3"></script>
<script type="text/javascript" src="ajax.js-jsver=2.1.1.3626.3" tppabs="http://i.renjihe.com/blog/images/js/ajax.js?jsver=2.1.1.3626.3"></script>
<script type="text/javascript" src="swfobject.js-jsver=2.1.1.3626.3" tppabs="http://i.renjihe.com/blog/images/js/swfobject.js?jsver=2.1.1.3626.3"></script>
<script type="text/javascript">
//<![CDATA[
var moreimagepath="template/default/images";
var shutajax=0;
var absbaseurl='index.htm'/*tpa=http://i.renjihe.com/blog/*/;
//]]>
</script>
<link title="搜索 流浪的龙－个人知识管理" rel="search"  type="application/opensearchdescription+xml"  href="http://i.renjihe.com/blog/inc/opensearch.php" />



</head>
<body id="pagelocation-read">
<div id="wrapper">
	<div id="innerWrapper">
		<div id="header">
			<div id="innerHeader">
				<div id="blogLogo">
				</div>
				<div class="blog-header">
					<h1 class="blog-title"><a href="index.php.htm" tppabs="http://i.renjihe.com/blog/index.php">流浪的龙－个人知识管理</a></h1>
					<div class="blog-desc"></div>
				</div>
				<div id="menu">
					<ul>
					<li><span id="nav_index" class="activepage"><a href="index.php.htm" tppabs="http://i.renjihe.com/blog/index.php" ><span id="navitem_index" class="activepageitem">首页</span></a></span></li>
<li><span id="nav_http_3A_2F_2Fi.renjihe.com_2F"><a href="javascript:if(confirm('http://i.renjihe.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://i.renjihe.com/'" tppabs="http://i.renjihe.com/"  target="_blank"><span id="navitem_http_3A_2F_2Fi.renjihe.com_2F">个性首页</span></a></span></li>
<li><span id="nav_http_3A_2F_2Fi.renjihe.com_2Flink.html"><a href="javascript:if(confirm('http://i.renjihe.com/link.html  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://i.renjihe.com/link.html'" tppabs="http://i.renjihe.com/link.html"  target="_blank"><span id="navitem_http_3A_2F_2Fi.renjihe.com_2Flink.html">网址导航</span></a></span></li>
<li><span id="nav_tag"><a href="tag.php.htm" tppabs="http://i.renjihe.com/blog/tag.php" ><span id="navitem_tag">标签</span></a></span></li>
<li><span id="nav_guestbook"><a href="guestbook.php.htm" tppabs="http://i.renjihe.com/blog/guestbook.php" ><span id="navitem_guestbook">留言</span></a></span></li>
					</ul>
				</div>
			</div>
		</div>
		<div id="mainWrapper">
			<div id="content" class="content">
				<div id="innerContent">
					<div class="announce" style="display: none">
						<div class="announce-content">
						
						</div>
					</div>
					<div class="article-top" style="display: none">
						<div class="pages">
							
						</div>
					</div>
					<div class="article-top">
	<div class="prev-article"><a href="read.php-7434.htm" tppabs="http://i.renjihe.com/blog/read.php?7434" title="上一篇 视觉SLAM漫谈 (三)"><img src="toolbar_previous.gif" tppabs="http://i.renjihe.com/blog/template/default/images/toolbar_previous.gif" alt='' border='0'/>视觉SLAM漫谈 (三)</a></div>
	<div class="next-article"></div>
</div>
<div class="textbox">
	<div class="textbox-calendar">
	<span class="textbox-calendar-month">Aug</span>
	<span class="textbox-calendar-day">17</span>
	</div>
	<div class="textbox-title">
		<h4>
		大牛的《深度学习》笔记，Deep Learning速成教程 <span id="starid7476"><img src="unstarred.gif" tppabs="http://i.renjihe.com/blog/template/default/images/others/unstarred.gif" alt="" title="未加星标" border="0"/></span> <img src="blank.gif" tppabs="http://i.renjihe.com/blog/images/weather/blank.gif" alt="不指定" title="不指定"/> 
		</h4>
		<div class="textbox-label">
		 <a href="view.php-go=user_1.htm" tppabs="http://i.renjihe.com/blog/view.php?go=user_1" target="_blank">renjihe1988</a> , 21:33 , <a href="index.php-go=category_37.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_37" title="查看分类： 理论">理论</a> &raquo; <a href="index.php-go=category_35.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_35" title="查看分类： AR&amp;AI&amp;模式识别">AR&amp;AI&amp;模式识别</a> , <a href="read.php-7476.htm#reply" tppabs="http://i.renjihe.com/blog/read.php?7476#reply" title="发表您的评论">评论(0)</a> , <a href='javascript: void(0);' title="查看引用地址" onclick='showhidediv("tb7476");'>引用(0)</a> , <a href="read.php-7476.htm" tppabs="http://i.renjihe.com/blog/read.php?7476">阅读(1)</a> ,  Via 本站原创 <span class="text-label-indented"><img src="toolbar_fontsize.gif" tppabs="http://i.renjihe.com/blog/template/default/images/toolbar_fontsize.gif" alt='' title="字体大小" border='0'/> <a href="javascript: doZoom(16);">大</a> | <a href="javascript: doZoom(14);">中</a> | <a href="javascript: doZoom(12);">小</a> <a href="javascript:if(confirm('http://i.renjihe.com/blog/read.php?7360  \n\nļ޷ Teleport Ultra , Ϊ , , Ŀֹͣ  \n\nڷϴ?'))window.location='http://i.renjihe.com/blog/read.php?7360'" tppabs="http://i.renjihe.com/blog/feed.php?go=entry_7476"><img src="toolbar_rss.gif" tppabs="http://i.renjihe.com/blog/template/default/images/toolbar_rss.gif" alt='' title="订阅本文" border='0'/></a> <a href="javascript:if(confirm('http://i.renjihe.com/blog/read.php?7359  \n\nļ޷ Teleport Ultra , Ϊ , , Ŀֹͣ  \n\nڷϴ?'))window.location='http://i.renjihe.com/blog/read.php?7359'" tppabs="http://i.renjihe.com/blog/read.php?save_7476"><img src="toolbar_save.gif" tppabs="http://i.renjihe.com/blog/template/default/images/toolbar_save.gif" alt='' title="保存本文为文本文档" border='0'/></a></span>
		</div>
	</div>
	<div id="tb7476" style="display: none;" class="textbox-tburl"><strong>引用功能被关闭了。</strong></div>
	
	<div class="textbox-content" id="zoomtext">
	 	http://www.leiphone.com/news/201608/7lwVZCXnScbQb6cJ.html<br /><br /><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="color: #7f7f7f"><em style="font-weight: normal">雷锋网<span style="color: #fd5d3c">(搜索&ldquo;雷锋网&rdquo;公众号关注)</span>按：本文由Zouxy责编，全面介绍了深度学习的发展历史及其在各个领域的应用，并解释了深度学习的基本思想，深度与浅度学习的区别和深度学习与<a style="text-decoration: none; color: #ed5565; border: 0px; margin: 0px; padding: 0px; vertical-align: baseline; transition: all 0.2s ease-out" href="javascript:if(confirm('http://www.leiphone.com/news/201505/read.php?7358  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.leiphone.com/news/201505/read.php?7358'" tppabs="http://www.leiphone.com/news/201505/t3T1XQy2g3spCUdd.html" target="_blank" title="神经网络">神经网络</a>之间的关系。</em></span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #3f3f3f"><em>深度学习</em></span><em><em style="line-height: 1.8"><span style="line-height: 1.8">，</span></em></em><em style="line-height: 1.8">即Deep Learning,是一种学习算法（Learning algorithm）,亦是人工智能领域的一个重要分支。从快速发展到实际应用，短短几年时间里，深度学习颠覆了语音识别、图像分类、文本理解等众多领域的算法设计思路，渐渐形成了一种从训练数据出发，经过一个端到端（end-to-end）的模型，然后直接输出得到最终结果的一种新模式。那么，深度学习有多深？学了究竟有几分？本文将带你领略深度学习高端范儿背后的方法与过程。</em></p><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'"><span style="color: #3f3f3f">一、概述</span><span style="color: #4f81bd"><br /></span></h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'"><span style="color: #3f3f3f">二、背景</span></h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'"><span style="color: #3f3f3f">三、人脑视觉机理</span></h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'"><span style="color: #3f3f3f">四、关于特征</span><br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.1、特征表示的粒度</h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.2、初级（浅层）特征表示</h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.3、结构性特征表示</h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.4、需要有多少个特征？</h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'"><span style="color: #3f3f3f">五、Deep Learning的基本思想</span></h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'"><span style="color: #3f3f3f">六、浅层学习（Shallow Learning）和深度学习（Deep Learning）</span></h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'"><span style="color: #3f3f3f">七、Deep learning与Neural Network</span></h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'"><span style="color: #3f3f3f">八、Deep learning训练过程</span></h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.1、传统神经网络的训练方法</h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.2、deep learning训练过程</h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'"><span style="color: #3f3f3f">九、Deep Learning的常用模型或者方法</span></h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9.1、AutoEncoder自动编码器</h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9.2、Sparse Coding稀疏编码</h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9.3、Restricted Boltzmann Machine(RBM)限制波尔兹曼机</h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9.4、Deep BeliefNetworks深信度网络</h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9.5、Convolutional Neural Networks卷积神经网络</h5><h5 style="margin: 20px 0px 15px; padding: 0px; font-size: 13px; color: #5a5a5a; font-family: 'microsoft yahei'"><span style="color: #3f3f3f">十、总结与展望</span></h5><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"></p><hr style="color: #5a5a5a; font-family: 'microsoft yahei'; font-size: 16px; line-height: 28.8px" /><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="font-size: 24px"><strong><span style="color: #ff0000">&#124;</span></strong></span><strong><span style="color: #000000">一、概述</span></strong><br /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">Artificial Intelligence，也就是人 &nbsp; 工智能，就像长生不老和星际漫游一样，是人类最美好的梦想之一。虽然计算机技术已经取得了长足的进步，但是到目前为止，还没有一台电脑能产生&ldquo;自我&rdquo;的意识。是的，在人类和大量现成数据的帮助下，电脑可以表现的十分强大，但是离开了这两者，它甚至都不能分辨一个喵星人和一个汪星人。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">图灵（图灵，大家都知道吧。计算机和人工智能的鼻祖，分别对应于其著名的&ldquo;图灵机&rdquo;和&ldquo;图灵测试&rdquo;）在 1950 年的论文里，提出图灵试验的设想，即，隔墙对话，你将不知道与你谈话的，是人还是电脑。这无疑给计算机，尤其是人工智能，预设了一个很高的期望值。但是半个世纪过去了，人工智能的进展，远远没有达到图灵试验的标准。这不仅让多年翘首以待的人们，心灰意冷，认为人工智能是忽悠，相关领域是&ldquo;伪科学&rdquo;。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">但是自 2006 年以来，机器学习领域，取得了突破性的进展。图灵试验，至少不是那么可望而不可及了。至于技术手段，不仅仅依赖于云计算对大数据的并行处理能力，而且依赖于算法。这个算法就是，Deep Learning。借助于 Deep Learning 算法，人类终于找到了如何处理&ldquo;抽象概念&rdquo;这个亘古难题的方法。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7357" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a1949a44b00.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><br /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">2012年6月，《纽约时报》披露了Google Brain项目，吸引了公众的广泛关注。这个项目是由著名的斯坦福大学的机器学习教授Andrew Ng和在大规模计算机系统方面的世界顶尖专家JeffDean共同主导，用16000个CPU Core的并行计算平台训练一种称为&ldquo;深度神经网络&rdquo;（DNN，Deep Neural Networks）的机器学习模型（内部共有10亿个节点。这一网络自然是不能跟人类的神经网络相提并论的。要知道，人脑中可是有150多亿个神经元，互相连接的节点也就是突触数更是如银河沙数。曾经有人估算过，如果将一个人的大脑中所有神经细胞的轴突和树突依次连接起来，并拉成一根直线，可从地球连到月亮，再从月亮返回地球），在语音识别和图像识别等领域获得了巨大的成功。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">项目负责人之一Andrew称：&ldquo;我们没有像通常做的那样自己框定边界，而是直接把海量数据投放到算法中，让数据自己说话，系统会自动从数据中学习。&rdquo;另外一名负责人Jeff则说：&ldquo;我们在训练的时候从来不会告诉机器说：&lsquo;这是一只猫。&rsquo;系统其实是自己发明或者领悟了&ldquo;猫&rdquo;的概念。&rdquo;</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7356" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194a6d11b3.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">2012年11月，微软在中国天津的一次活动上公开演示了一个全自动的同声传译系统，讲演者用英文演讲，后台的计算机一气呵成自动完成语音识别、英中机器翻译和中文语音合成，效果非常流畅。据报道，后面支撑的关键技术也是DNN，或者深度学习（DL，DeepLearning）。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">2013年1月，在百度年会上，创始人兼CEO李彦宏高调宣布要成立百度研究院，其中第一个成立的就是&ldquo;深度学习研究所&rdquo;（IDL，Institue of Deep Learning）。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7355" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194a894f79.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><br /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">为什么拥有大数据的互联网公司争相投入大量资源研发深度学习技术。听起来感觉deeplearning很牛那样。那什么是deep learning？为什么有deep learning？它是怎么来的？又能干什么呢？目前存在哪些困难呢？这些问题的简答都需要慢慢来。咱们先来了解下机器学习（人工智能的核心）的背景。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="font-size: 24px"><strong><span style="color: #ff0000">&#124;</span></strong></span><strong><span style="color: #0c0c0c">二、背景</span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">机器学习（Machine Learning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。机器能否像人类一样能具有学习能力呢？1959年美国的塞缪尔(Samuel)设计了一个下棋程序，这个程序具有学习能力，它可以在不断的对弈中改善自己的棋艺。4年后，这个程序战胜了设计者本人。又过了3年，这个程序战胜了美国一个保持8年之久的常胜不败的冠军。这个程序向人们展示了机器学习的能力，提出了许多令人深思的社会问题与哲学问题（呵呵，人工智能正常的轨道没有很大的发展，这些什么哲学伦理啊倒发展的挺快。什么未来机器越来越像人，人越来越像机器啊。什么机器会反人类啊，ATM是开第一枪的啊等等。人类的思维无穷啊）。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">机器学习虽然发展了几十年，但还是存在很多没有良好解决的问题：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7354" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194aa12a0b.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><br /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">例如图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等等。目前我们通过机器学习去解决这些问题的思路都是这样的（以视觉感知为例子）：<br /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7353" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194ab9d5b5.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">从开始的通过传感器（例如CMOS）来获得数据。然后经过预处理、特征提取、特征选择，再到推理、预测或者识别。最后一个部分，也就是机器学习的部分，绝大部分的工作是在这方面做的，也存在很多的paper和研究。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">而中间的三部分，概括起来就是特征表达。良好的特征表达，对最终算法的准确性起了非常关键的作用，而且系统主要的计算和测试工作都耗在这一大部分。但，这块实际中一般都是人工完成的。靠人工提取特征。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7352" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194adb59f3.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">截止现在，也出现了不少NB的特征（好的特征应具有不变性（大小、尺度和旋转等）和可区分性）：例如Sift的出现，是局部图像特征描述子研究领域一项里程碑式的工作。由于SIFT对尺度、旋转以及一定视角和光照变化等图像变化都具有不变性，并且SIFT具有很强的可区分性，的确让很多问题的解决变为可能。但它也不是万能的。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7351" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194af44009.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">然而，手工地选取特征是一件非常费力、启发式（需要专业知识）的方法，能不能选取好很大程度上靠经验和运气，而且它的调节需要大量的时间。既然手工选取特征不太好，那么能不能自动地学习一些特征呢？答案是能！Deep Learning就是用来干这个事情的，看它的一个别名UnsupervisedFeature Learning，就可以顾名思义了，Unsupervised的意思就是不要人参与特征的选取过程。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">那它是怎么学习的呢？怎么知道哪些特征好哪些不好呢？我们说机器学习是一门专门研究计算机怎样模拟或实现人类的学习行为的学科。好，那我们人的视觉系统是怎么工作的呢？为什么在茫茫人海，芸芸众生，滚滚红尘中我们都可以找到另一个她（因为，你存在我深深的脑海里，我的梦里 我的心里 我的歌声里&hellip;&hellip;）。人脑那么NB，我们能不能参考人脑，模拟人脑呢？（好像和人脑扯上点关系的特征啊，算法啊，都不错，但不知道是不是人为强加的，为了使自己的作品变得神圣和高雅。） 近几十年以来，认知神经科学、生物学等等学科的发展，让我们对自己这个神秘的而又神奇的大脑不再那么的陌生。也给人工智能的发展推波助澜。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="font-size: 24px; color: #ff0000">&#124;</span><span style="color: #0c0c0c">三、人脑视觉机理</span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">1981 年的诺贝尔医学奖，颁发给了 David Hubel（出生于加拿大的美国神经生物学家） 和TorstenWiesel，以及 Roger Sperry。前两位的主要贡献，是&ldquo;发现了视觉系统的信息处理&rdquo;：可视皮层是分级的：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7350" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194b0b1b64.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">我们看看他们做了什么。1958 年，DavidHubel 和Torsten Wiesel 在 JohnHopkins University，研究瞳孔区域与大脑皮层神经元的对应关系。他们在猫的后脑头骨上，开了一个3 毫米的小洞，向洞里插入电极，测量神经元的活跃程度。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">然后，他们在小猫的眼前，展现各种形状、各种亮度的物体。并且，在展现每一件物体时，还改变物体放置的位置和角度。他们期望通过这个办法，让小猫瞳孔感受不同类型、不同强弱的刺激。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">之所以做这个试验，目的是去证明一个猜测。位于后脑皮层的不同视觉神经元，与瞳孔所受刺激之间，存在某种对应关系。一旦瞳孔受到某一种刺激，后脑皮层的某一部分神经元就会活跃。经历了很多天反复的枯燥的试验，同时牺牲了若干只可怜的小猫，David Hubel 和Torsten Wiesel 发现了一种被称为&ldquo;方向选择性细胞（Orientation Selective Cell）&rdquo;的神经元细胞。当瞳孔发现了眼前的物体的边缘，而且这个边缘指向某个方向时，这种神经元细胞就会活跃。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">这个发现激发了人们对于神经系统的进一步思考。神经-中枢-大脑的工作过程，或许是一个不断迭代、不断抽象的过程。这里的关键词有两个，一个是抽象，一个是迭代。从原始信号，做低级抽象，逐渐向高级抽象迭代。人类的逻辑思维，经常使用高度抽象的概念。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">例如，从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7349" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194b23a6e1.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">这个生理学的发现，促成了计算机人工智能，在四十年后的突破性发展。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">总的来说，人的视觉系统的信息处理是分级的。从低级的V1区提取边缘特征，再到V2区的形状或者目标的部分等，再到更高层，整个目标、目标的行为等。也就是说高层的特征是低层特征的组合，从低层到高层的特征表示越来越抽象，越来越能表现语义或者意图。而抽象层面越高，存在的可能猜测就越少，就越利于分类。例如，单词集合和句子的对应是多对一的，句子和语义的对应又是多对一的，语义和意图的对应还是多对一的，这是个层级体系。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">敏感的人注意到关键词了：分层。而Deep learning的deep是不是就表示我存在多少层，也就是多深呢？没错。那Deep learning是如何借鉴这个过程的呢？毕竟是归于计算机来处理，面对的一个问题就是怎么对这个过程建模？</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">因为我们要学习的是特征的表达，那么关于特征，或者说关于这个层级特征，我们需要了解地更深入点。所以在说Deep Learning之前，我们有必要再啰嗦下特征（呵呵，实际上是看到那么好的对特征的解释，不放在这里有点可惜，所以就塞到这了）。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="font-size: 24px; color: #ff0000"><strong>&#124;</strong></span><span style="color: #0c0c0c">四、关于特征</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">特征是机器学习系统的原材料，对最终模型的影响是毋庸置疑的。如果数据被很好的表达成了特征，通常线性模型就能达到满意的精度。那对于特征，我们需要考虑什么呢？</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #0c0c0c">4.1、特征表示的粒度</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">学习算法在一个什么粒度上的特征表示，才有能发挥作用-？就一个图片来说，像素级的特征根本没有价值。例如下面的摩托车，从像素级别，根本得不到任何信息，其无法进行摩托车和非摩托车的区分。而如果特征是一个具有结构性（或者说有含义）的时候，比如是否具有车把手（handle），是否具有车轮（wheel），就很容易把摩托车和非摩托车区分，学习算法才能发挥作用。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7348" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194b422c8b.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #0c0c0c">4.2、初级（浅层）特征表示</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #0c0c0c">既然像素级的特征表示方法没有作用，那怎样的表示才有用呢？</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">1995 年前后，Bruno Olshausen和 David Field 两位学者任职 Cornell University，他们试图同时用生理学和计算机的手段，双管齐下，研究视觉问题。 他们收集了很多黑白风景照片，从这些照片中，提取出400个小碎片，每个照片碎片的尺寸均为 16x16 像素，不妨把这400个碎片标记为 S[i], i = 0,.. 399。接下来，再从这些黑白风景照片中，随机提取另一个碎片，尺寸也是 16x16 像素，不妨把这个碎片标记为 T。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">他们提出的问题是，如何从这400个碎片中，选取一组碎片，S[k], 通过叠加的办法，合成出一个新的碎片，而这个新的碎片，应当与随机选择的目标碎片 T，尽可能相似，同时，S[k] 的数量尽可能少。用数学的语言来描述，就是：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #7f7f7f"><em>Sum_k (a[k] * S[k]) --&gt; T,&nbsp;&nbsp;&nbsp;&nbsp; 其中 a[k] 是在叠加碎片 S[k] 时的权重系数。</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #7f7f7f"><em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 为解决这个问题，Bruno Olshausen和 David Field 发明了一个算法，稀疏编码（Sparse Coding）。</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">稀疏编码是一个重复迭代的过程，每次迭代分两步：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><em><span style="color: #7f7f7f">1）选择一组 S[k]，然后调整 a[k]，使得Sum_k (a[k] * S[k]) 最接近 T。</span></em></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><em><span style="color: #7f7f7f">2）固定住 a[k]，在 400 个碎片中，选择其它更合适的碎片S&rsquo;[k]，替代原先的 S[k]，使得Sum_k (a[k] * S&rsquo;[k]) 最接近 T。</span></em></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">&nbsp;经过几次迭代后，最佳的 S[k] 组合，被遴选出来了。令人惊奇的是，被选中的 S[k]，基本上都是照片上不同物体的边缘线，这些线段形状相似，区别在于方向。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">Bruno Olshausen和 David Field 的算法结果，与 David Hubel 和Torsten Wiesel 的生理发现，不谋而合！</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">也就是说，复杂图形，往往由一些基本结构组成。比如下图：一个图可以通过用64种正交的edges（可以理解成正交的基本结构）来线性表示。比如样例的x可以用1-64个edges中的三个按照0.8,0.3,0.5的权重调和而成。而其他基本edge没有贡献，因此均为0 。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7347" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194b62d876.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">另外，大牛们还发现，不仅图像存在这个规律，声音也存在。他们从未标注的声音中发现了20种基本的声音结构，其余的声音可以由这20种基本结构合成。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7346" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194bd723b9.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="line-height: 1.8; color: #0c0c0c">4.3、结构性特征表示</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">小块的图形可以由基本edge构成，更结构化，更复杂的，具有概念性的图形如何表示呢？这就需要更高层次的特征表示，比如V2，V4。因此V1看像素级是像素级。V2看V1是像素级，这个是层次递进的，高层表达由底层表达的组合而成。专业点说就是基basis。V1取提出的basis是边缘，然后V2层是V1层这些basis的组合，这时候V2区得到的又是高一层的basis。即上一层的basis组合的结果，上上层又是上一层的组合basis&hellip;&hellip;（所以有大牛说Deep learning就是&ldquo;搞基&rdquo;，因为难听，所以美其名曰Deep learning或者Unsupervised Feature Learning）</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7345" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194bf934d3.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">&nbsp;直观上说，就是找到make sense的小patch再将其进行combine，就得到了上一层的feature，递归地向上learning feature。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">在不同object上做training是，所得的edge basis 是非常相似的，但object parts和models 就会completely different了（那咱们分辨car或者face是不是容易多了）：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7344" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194c11bdbb.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">从文本来说，一个doc表示什么意思？我们描述一件事情，用什么来表示比较合适？用一个一个字嘛，我看不是，字就是像素级别了，起码应该是term，换句话说每个doc都由term构成，但这样表示概念的能力就够了嘛，可能也不够，需要再上一步，达到topic级，有了topic，再到doc就合理。但每个层次的数量差距很大，比如doc表示的概念-&gt;topic（千-万量级）-&gt;term（10万量级）-&gt;word（百万量级）。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">一个人在看一个doc的时候，眼睛看到的是word，由这些word在大脑里自动切词形成term，在按照概念组织的方式，先验的学习，得到topic，然后再进行高层次的learning。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #0c0c0c">4.4、需要有多少个特征？</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">我们知道需要层次的特征构建，由浅入深，但每一层该有多少个特征呢？</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">任何一种方法，特征越多，给出的参考信息就越多，准确性会得到提升。但特征多意味着计算复杂，探索的空间大，可以用来训练的数据在每个特征上就会稀疏，都会带来各种问题，并不一定特征越多越好。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7343" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194c332598.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><br /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">好了，到了这一步，终于可以聊到Deep learning了。上面我们聊到为什么会有Deep learning（让机器自动学习良好的特征，而免去人工选取过程。还有参考人的分层视觉处理系统），我们得到一个结论就是Deep learning需要多层来获得更抽象的特征表达。那么多少层才合适呢？用什么架构来建模呢？怎么进行非监督训练呢？</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="font-size: 24px; color: #ff0000">&#124;</span></strong><span style="color: #0c0c0c">五、Deep Learning的基本思想</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">假设我们有一个系统S，它有n层（S1,&hellip;Sn），它的输入是I，输出是O，形象地表示为： I =&gt;S1=&gt;S2=&gt;&hellip;..=&gt;Sn =&gt; O，如果输出O等于输入I，即输入I经过这个系统变化之后没有任何的信息损失（呵呵，大牛说，这是不可能的。信息论中有个&ldquo;信息逐层丢失&rdquo;的说法（信息处理不等式），设处理a信息得到b，再对b处理得到c，那么可以证明：a和c的互信息不会超过a和b的互信息。这表明信息处理不会增加信息，大部分处理会丢失信息。当然了，如果丢掉的是没用的信息那多好啊），保持了不变，这意味着输入I经过每一层Si都没有任何的信息损失，即在任何一层Si，它都是原有信息（即输入I）的另外一种表示。现在回到我们的主题Deep Learning，我们需要自动地学习特征，假设我们有一堆输入I（如一堆图像或者文本），假设我们设计了一个系统S（有n层），我们通过调整系统中参数，使得它的输出仍然是输入I，那么我们就可以自动地获取得到输入I的一系列层次特征，即S1，&hellip;, Sn。<br /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">对于深度学习来说，其思想就是对堆叠多个层，也就是说这一层的输出作为下一层的输入。通过这种方式，就可以实现对输入信息进行分级表达了。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">另外，前面是假设输出严格地等于输入，这个限制太严格，我们可以略微地放松这个限制，例如我们只要使得输入与输出的差别尽可能地小即可，这个放松会导致另外一类不同的Deep Learning方法。上述就是Deep Learning的基本思想。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="font-size: 24px; color: #ff0000">&#124;</span><span style="color: #0c0c0c">六、浅层学习（Shallow Learning）和深度学习（Deep Learning）</span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><span style="color: #0c0c0c">浅层学习是机器学习的第一次浪潮。</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">20世纪80年代末期，用于人工神经网络的反向传播算法（也叫Back Propagation算法或者BP算法）的发明，给机器学习带来了希望，掀起了基于统计模型的机器学习热潮。这个热潮一直持续到今天。人们发现，利用BP算法可以让一个人工神经网络模型从大量训练样本中学习统计规律，从而对未知事件做预测。这种基于统计的机器学习方法比起过去基于人工规则的系统，在很多方面显出优越性。这个时候的人工神经网络，虽也被称作多层感知机（Multi-layer Perceptron），但实际是种只含有一层隐层节点的浅层模型。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">20世纪90年代，各种各样的浅层机器学习模型相继被提出，例如支撑向量机（SVM，Support Vector Machines）、 Boosting、最大熵方法（如LR，Logistic Regression）等。这些模型的结构基本上可以看成带有一层隐层节点（如SVM、Boosting），或没有隐层节点（如LR）。这些模型无论是在理论分析还是应用中都获得了巨大的成功。相比之下，由于理论分析的难度大，训练方法又需要很多经验和技巧，这个时期浅层人工神经网络反而相对沉寂。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><span style="color: #0c0c0c">&nbsp;深度学习是机器学习的第二次浪潮。</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">2006年，加拿大多伦多大学教授、机器学习领域的泰斗Geoffrey Hinton和他的学生RuslanSalakhutdinov在《科学》上发表了一篇文章，开启了深度学习在学术界和工业界的浪潮。这篇文章有两个主要观点：1）多隐层的人工神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；2）深度神经网络在训练上的难度，可以通过&ldquo;逐层初始化&rdquo;（layer-wise pre-training）来有效克服，在这篇文章中，逐层初始化是通过无监督学习实现的。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">当前多数分类、回归等学习方法为浅层结构算法，其局限性在于有限样本和计算单元情况下对复杂函数的表示能力有限，针对复杂分类问题其泛化能力受到一定制约。深度学习可通过学习一种深层非线性网络结构，实现复杂函数逼近，表征输入数据分布式表示，并展现了强大的从少数样本集中学习数据集本质特征的能力。（多层的好处是可以用较少的参数表示复杂的函数）</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7342" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194c4ac548.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">深度学习的实质，是通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。因此，&ldquo;深度模型&rdquo;是手段，&ldquo;特征学习&rdquo;是目的。区别于传统的浅层学习，深度学习的不同在于：1）强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点；2）明确突出了特征学习的重要性，也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据的丰富内在信息。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="font-size: 24px; color: #ff0000">&#124;</span><span style="color: #0c0c0c">七、Deep learning与Neural Network</span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。深度学习是无监督学习的一种。<br /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">Deep learning本身算是machine learning的一个分支，简单可以理解为neural network的发展。大约二三十年前，neural network曾经是ML领域特别火热的一个方向，但是后来确慢慢淡出了，原因包括以下几个方面：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #7f7f7f"><em>1）比较容易过拟合，参数比较难tune，而且需要不少trick；</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #7f7f7f"><em>2）训练速度比较慢，在层次比较少（小于等于3）的情况下效果并不比其它方法更优；</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">所以中间有大约20多年的时间，神经网络被关注很少，这段时间基本上是SVM和boosting算法的天下。但是，一个痴心的老先生Hinton，他坚持了下来，并最终（和其它人一起Bengio、Yann.lecun等）提成了一个实际可行的deep learning框架。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #000000">Deep learning与传统的神经网络之间有相同的地方也有很多不同。</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">二者的相同在于deep learning采用了神经网络相似的分层结构，系统由包括输入层、隐层（多层）、输出层组成的多层网络，只有相邻层节点之间有连接，同一层以及跨层节点之间相互无连接，每一层可以看作是一个logistic regression模型；这种分层结构，是比较接近人类大脑的结构的。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7341" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194c60b147.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><br /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">而为了克服神经网络训练中的问题，DL采用了与神经网络很不同的训练机制。传统神经网络中，采用的是back propagation的方式进行，简单来讲就是采用迭代的算法来训练整个网络，随机设定初值，计算当前网络的输出，然后根据当前输出和label之间的差去改变前面各层的参数，直到收敛（整体是一个梯度下降法）。而deep learning整体上是一个layer-wise的训练机制。这样做的原因是因为，如果采用back propagation的机制，对于一个deep network（7层以上），残差传播到最前面的层已经变得太小，出现所谓的gradient diffusion（梯度扩散）。这个问题我们接下来讨论。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="font-size: 24px; color: #ff0000">&#124;</span><span style="color: #000000">八、Deep learning训练过程</span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #000000">8.1、传统神经网络的训练方法为什么不能用在深度神经网络</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">BP算法作为传统训练多层网络的典型算法，实际上对仅含几层网络，该训练方法就已经很不理想。深度结构（涉及多个非线性处理单元层）非凸目标代价函数中普遍存在的局部最小是训练困难的主要来源。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><em style="font-weight: normal"><span style="color: #7f7f7f">BP算法存在的问题：</span></em></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><em><span style="color: #7f7f7f">（1）梯度越来越稀疏：从顶层越往下，误差校正信号越来越小；</span></em></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><em><span style="color: #7f7f7f">（2）收敛到局部最小值：尤其是从远离最优区域开始的时候（随机值初始化会导致这种情况的发生）；</span></em></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><em><span style="color: #7f7f7f">（3）一般，我们只能用有标签的数据来训练：但大部分的数据是没标签的，而大脑可以从没有标签的的数据中学习；</span></em></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #000000">8.2、deep learning训练过程</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">如果对所有层同时训练，时间复杂度会太高；如果每次训练一层，偏差就会逐层传递。这会面临跟上面监督学习中相反的问题，会严重欠拟合（因为深度网络的神经元和参数太多了）。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">2006年，hinton提出了在非监督数据上建立多层神经网络的一个有效方法，简单的说，分为两步，一是每次训练一层网络，二是调优，使原始表示x向上生成的高级表示r和该高级表示r向下生成的x'尽可能一致。方法是：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #7f7f7f"><em>1）首先逐层构建单层神经元，这样每次都是训练一个单层网络。</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #7f7f7f"><em>2）当所有层训练完后，Hinton使用wake-sleep算法进行调优。</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">将除最顶层的其它层间的权重变为双向的，这样最顶层仍然是一个单层神经网络，而其它层则变为了图模型。向上的权重用于&ldquo;认知&rdquo;，向下的权重用于&ldquo;生成&rdquo;。然后使用Wake-Sleep算法调整所有的权重。让认知和生成达成一致，也就是保证生成的最顶层表示能够尽可能正确的复原底层的结点。比如顶层的一个结点表示人脸，那么所有人脸的图像应该激活这个结点，并且这个结果向下生成的图像应该能够表现为一个大概的人脸图像。Wake-Sleep算法分为醒（wake）和睡（sleep）两个部分。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #7f7f7f"><em>1）wake阶段：认知过程，通过外界的特征和向上的权重（认知权重）产生每一层的抽象表示（结点状态），并且使用梯度下降修改层间的下行权重（生成权重）。也就是&ldquo;如果现实跟我想象的不一样，改变我的权重使得我想象的东西就是这样的&rdquo;。</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #7f7f7f"><em>2）sleep阶段：生成过程，通过顶层表示（醒时学得的概念）和向下权重，生成底层的状态，同时修改层间向上的权重。也就是&ldquo;如果梦中的景象不是我脑中的相应概念，改变我的认知权重使得这种景象在我看来就是这个概念&rdquo;。</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #000000">deep learning训练过程具体如下：</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="color: #3f3f3f"><em style="font-weight: normal">1）使用自下上升非监督学习（就是从底层开始，一层一层的往顶层训练）：</em></span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">采用无标定数据（有标定数据也可）分层训练各层参数，这一步可以看作是一个无监督训练过程，是和传统神经网络区别最大的部分（这个过程可以看作是feature learning过程）</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">具体的，先用无标定数据训练第一层，训练时先学习第一层的参数（这一层可以看作是得到一个使得输出和输入差别最小的三层神经网络的隐层），由于模型capacity的限制以及稀疏性约束，使得得到的模型能够学习到数据本身的结构，从而得到比输入更具有表示能力的特征；在学习得到第n-1层后，将n-1层的输出作为第n层的输入，训练第n层，由此分别得到各层的参数；</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="color: #3f3f3f"><em style="font-weight: normal">2）自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）：</em></span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">基于第一步得到的各层参数进一步fine-tune整个多层模型的参数，这一步是一个有监督训练过程；第一步类似神经网络的随机初始化初值过程，由于DL的第一步不是随机初始化，而是通过学习输入数据的结构得到的，因而这个初值更接近全局最优，从而能够取得更好的效果；所以deep learning效果好很大程度上归功于第一步的feature learning过程。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="font-size: 24px; color: #ff0000">&#124;</span><span style="color: #000000">九、Deep Learning的常用模型或者方法</span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #000000">9.1、AutoEncoder自动编码器</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">Deep Learning最简单的一种方法是利用人工神经网络的特点，人工神经网络（ANN）本身就是具有层次结构的系统，如果给定一个神经网络，我们假设其输出与输入是相同的，然后训练调整其参数，得到每一层中的权重。自然地，我们就得到了输入I的几种不同表示（每一层代表一种表示），这些表示就是特征。自动编码器就是一种尽可能复现输入信号的神经网络。为了实现这种复现，自动编码器就必须捕捉可以代表输入数据的最重要的因素，就像PCA那样，找到可以代表原信息的主要成分。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #000000">具体过程简单的说明如下：</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="color: #3f3f3f"><em style="font-weight: normal">1）给定无标签数据，用非监督学习学习特征：</em></span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><em><span style="color: #262626"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7340" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194c780cc3.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></span></em></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><br /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">在我们之前的神经网络中，如第一个图，我们输入的样本是有标签的，即（input, target），这样我们根据当前输出和target（label）之间的差去改变前面各层的参数，直到收敛。但现在我们只有无标签数据，也就是右边的图。那么这个误差怎么得到呢？</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7339" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194c8aaccd.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><br /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">如上图，我们将input输入一个encoder编码器，就会得到一个code，这个code也就是输入的一个表示，那么我们怎么知道这个code表示的就是input呢？我们加一个decoder解码器，这时候decoder就会输出一个信息，那么如果输出的这个信息和一开始的输入信号input是很像的（理想情况下就是一样的），那很明显，我们就有理由相信这个code是靠谱的。所以，我们就通过调整encoder和decoder的参数，使得重构误差最小，这时候我们就得到了输入input信号的第一个表示了，也就是编码code了。因为是无标签数据，所以误差的来源就是直接重构后与原输入相比得到。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7338" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194c9d65f0.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="color: #3f3f3f"><em style="font-weight: normal">2）通过编码器产生特征，然后训练下一层。这样逐层训练：</em></span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">那上面我们就得到第一层的code，我们的重构误差最小让我们相信这个code就是原输入信号的良好表达了，或者牵强点说，它和原信号是一模一样的（表达不一样，反映的是一个东西）。那第二层和第一层的训练方式就没有差别了，我们将第一层输出的code当成第二层的输入信号，同样最小化重构误差，就会得到第二层的参数，并且得到第二层输入的code，也就是原输入信息的第二个表达了。其他层就同样的方法炮制就行了（训练这一层，前面层的参数都是固定的，并且他们的decoder已经没用了，都不需要了）。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7337" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194cab95cd.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="color: #3f3f3f"><em style="font-weight: normal">3）有监督微调：</em></span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">经过上面的方法，我们就可以得到很多层了。至于需要多少层（或者深度需要多少，这个目前本身就没有一个科学的评价方法）需要自己试验调了。每一层都会得到原始输入的不同的表达。当然了，我们觉得它是越抽象越好了，就像人的视觉系统一样。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">到这里，这个AutoEncoder还不能用来分类数据，因为它还没有学习如何去连结一个输入和一个类。它只是学会了如何去重构或者复现它的输入而已。或者说，它只是学习获得了一个可以良好代表输入的特征，这个特征可以最大程度上代表原输入信号。那么，为了实现分类，我们就可以在AutoEncoder的最顶的编码层添加一个分类器（例如罗杰斯特回归、SVM等），然后通过标准的多层神经网络的监督训练方法（梯度下降法）去训练。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">也就是说，这时候，我们需要将最后层的特征code输入到最后的分类器，通过有标签样本，通过监督学习进行微调，这也分两种，一个是只调整分类器（黑色部分）：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7336" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194cbd0665.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">另一种：通过有标签样本，微调整个系统：（如果有足够多的数据，这个是最好的。end-to-end learning端对端学习）</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7335" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194cced803.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">一旦监督训练完成，这个网络就可以用来分类了。神经网络的最顶层可以作为一个线性分类器，然后我们可以用一个更好性能的分类器去取代它。在研究中可以发现，如果在原有的特征中加入这些自动学习得到的特征可以大大提高精确度，甚至在分类问题中比目前最好的分类算法效果还要好！</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">AutoEncoder存在一些变体，这里简要介绍下两个：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="color: #3f3f3f"><em style="font-weight: normal">Sparse AutoEncoder稀疏自动编码器：</em></span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">当然，我们还可以继续加上一些约束条件得到新的Deep Learning方法，如：如果在AutoEncoder的基础上加上L1的Regularity限制（L1主要是约束每一层中的节点中大部分都要为0，只有少数不为0，这就是Sparse名字的来源），我们就可以得到Sparse AutoEncoder法。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7334" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a194ce144bc.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">如上图，其实就是限制每次得到的表达code尽量稀疏。因为稀疏的表达往往比其他的表达要有效（人脑好像也是这样的，某个输入只是刺激某些神经元，其他的大部分的神经元是受到抑制的）。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #3f3f3f"><em>Denoising AutoEncoders降噪自动编码器：</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">降噪自动编码器DA是在自动编码器的基础上，训练数据加入噪声，所以自动编码器必须学习去去除这种噪声而获得真正的没有被噪声污染过的输入。因此，这就迫使编码器去学习输入信号的更加鲁棒的表达，这也是它的泛化能力比一般编码器强的原因。DA可以通过梯度下降算法去训练。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7333" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197a4a7398.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #0c0c0c">9.2、Sparse Coding稀疏编码</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">如果我们把输出必须和输入相等的限制放松，同时利用线性代数中基的概念，即O = a1*&Phi;1&nbsp;+ a2*&Phi;2+&hellip;.+ an*&Phi;n， &Phi;i是基，ai是系数，我们可以得到这样一个优化问题：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">Min &#124;I &ndash; O&#124;，其中I表示输入，O表示输出。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">通过求解这个最优化式子，我们可以求得系数ai和基&Phi;i，这些系数和基就是输入的另外一种近似表达。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7332" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197a5a2f6d.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><br /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">因此，它们可以用来表达输入I，这个过程也是自动学习得到的。如果我们在上述式子上加上L1的Regularity限制，得到：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">Min &#124;I &ndash; O&#124; + u*(&#124;a1&#124; + &#124;a2&#124; + &hellip; + &#124;an&nbsp;&#124;)</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">这种方法被称为Sparse Coding。通俗的说，就是将一个信号表示为一组基的线性组合，而且要求只需要较少的几个基就可以将信号表示出来。&ldquo;稀疏性&rdquo;定义为：只有很少的几个非零元素或只有很少的几个远大于零的元素。要求系数 ai&nbsp;是稀疏的意思就是说：对于一组输入向量，我们只想有尽可能少的几个系数远大于零。选择使用具有稀疏性的分量来表示我们的输入数据是有原因的，因为绝大多数的感官数据，比如自然图像，可以被表示成少量基本元素的叠加，在图像中这些基本元素可以是面或者线。同时，比如与初级视觉皮层的类比过程也因此得到了提升（人脑有大量的神经元，但对于某些图像或者边缘只有很少的神经元兴奋，其他都处于抑制状态）。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">稀疏编码算法是一种无监督学习方法，它用来寻找一组&ldquo;超完备&rdquo;基向量来更高效地表示样本数据。虽然形如主成分分析技术（PCA）能使我们方便地找到一组&ldquo;完备&rdquo;基向量，但是这里我们想要做的是找到一组&ldquo;超完备&rdquo;基向量来表示输入向量（也就是说，基向量的个数比输入向量的维数要大）。超完备基的好处是它们能更有效地找出隐含在输入数据内部的结构与模式。然而，对于超完备基来说，系数ai不再由输入向量唯一确定。因此，在稀疏编码算法中，我们另加了一个评判标准&ldquo;稀疏性&rdquo;来解决因超完备而导致的退化（degeneracy）问题。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7331" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197a6a6c41.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">比如在图像的Feature Extraction的最底层要做Edge Detector的生成，那么这里的工作就是从Natural Images中randomly选取一些小patch，通过这些patch生成能够描述他们的&ldquo;基&rdquo;，也就是右边的8*8=64个basis组成的basis，然后给定一个test patch, 我们可以按照上面的式子通过basis的线性组合得到，而sparse matrix就是a，下图中的a中有64个维度，其中非零项只有3个，故称&ldquo;sparse&rdquo;。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">这里可能大家会有疑问，为什么把底层作为Edge Detector呢？上层又是什么呢？这里做个简单解释大家就会明白，之所以是Edge Detector是因为不同方向的Edge就能够描述出整幅图像，所以不同方向的Edge自然就是图像的basis了&hellip;&hellip;而上一层的basis组合的结果，上上层又是上一层的组合basis&hellip;&hellip;（就是上面第四部分的时候咱们说的那样）</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #0c0c0c">Sparse coding分为两个部分：</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #3f3f3f"><strong><em style="font-weight: normal">1）Training阶段：给定一系列的样本图片[x1, x 2, &hellip;]，我们需要学习得到一组基[&Phi;1, &Phi;2, &hellip;]，也就是字典。</em></strong></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">稀疏编码是k-means算法的变体，其训练过程也差不多（EM算法的思想：如果要优化的目标函数包含两个变量，如L(W, B)，那么我们可以先固定W，调整B使得L最小，然后再固定B，调整W使L最小，这样迭代交替，不断将L推向最小值。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">训练过程就是一个重复迭代的过程，按上面所说，我们交替的更改a和&Phi;使得下面这个目标函数最小。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7330" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197b2a6c53.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #1d1b10"><em>每次迭代分两步：</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #262626"><em>a）固定字典&Phi;[k]，然后调整a[k]，使得上式，即目标函数最小（即解LASSO问题）。</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #262626"><em>b）然后固定住a [k]，调整&Phi; [k]，使得上式，即目标函数最小（即解凸QP问题）。</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #262626"><em>不断迭代，直至收敛。这样就可以得到一组可以良好表示这一系列x的基，也就是字典。</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="color: #3f3f3f"><em style="font-weight: normal">2）Coding阶段：给定一个新的图片x，由上面得到的字典，通过解一个LASSO问题得到稀疏向量a。这个稀疏向量就是这个输入向量x的一个稀疏表达了。</em></span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><span style="color: #974806"><em><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7329" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197b3d1705.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">例如：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7328" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a1997696e8b.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="color: #0c0c0c">9.3、Restricted Boltzmann Machine (RBM)限制波尔兹曼机</span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">假设有一个二部图，每一层的节点之间没有链接，一层是可视层，即输入数据层（v)，一层是隐藏层(h)，如果假设所有的节点都是随机二值变量节点（只能取0或者1值），同时假设全概率分布p(v,h)满足Boltzmann 分布，我们称这个模型是Restricted BoltzmannMachine (RBM)。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7327" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197b64304c.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">下面我们来看看为什么它是Deep Learning方法。首先，这个模型因为是二部图，所以在已知v的情况下，所有的隐藏节点之间是条件独立的（因为节点之间不存在连接），即p(h&#124;v)=p(h1&#124;v)&hellip;p(hn&#124;v)。同理，在已知隐藏层h的情况下，所有的可视节点都是条件独立的。同时又由于所有的v和h满足Boltzmann 分布，因此，当输入v的时候，通过p(h&#124;v) 可以得到隐藏层h，而得到隐藏层h之后，通过p(v&#124;h)又能得到可视层，通过调整参数，我们就是要使得从隐藏层得到的可视层v1与原来的可视层v如果一样，那么得到的隐藏层就是可视层另外一种表达，因此隐藏层可以作为可视层输入数据的特征，所以它就是一种Deep Learning方法。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7326" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197b776f90.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">如何训练呢？也就是可视层节点和隐节点间的权值怎么确定呢？我们需要做一些数学分析。也就是模型了。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7325" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a19a3c4c75c.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">联合组态（jointconfiguration）的能量可以表示为：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7324" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197b8bce11.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">而某个组态的联合概率分布可以通过Boltzmann 分布（和这个组态的能量）来确定：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7323" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197b9d50e4.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">因为隐藏节点之间是条件独立的（因为节点之间不存在连接），即：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7322" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197bacf97d.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">然后我们可以比较容易（对上式进行因子分解Factorizes）得到在给定可视层v的基础上，隐层第j个节点为1或者为0的概率：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7321" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197bbaaf91.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">同理，在给定隐层h的基础上，可视层第i个节点为1或者为0的概率也可以容易得到：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7320" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197bc9b886.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">给定一个满足独立同分布的样本集：D=&#123;v(1),&nbsp;v(2),&hellip;,&nbsp;v(N)&#125;，我们需要学习参数&theta;=&#123;W,a,b&#125;。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">我们最大化以下对数似然函数（最大似然估计：对于某个概率模型，我们需要选择一个参数，让我们当前的观测样本的概率最大）：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7319" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197bd86eae.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">也就是对最大对数似然函数求导，就可以得到L最大时对应的参数W了。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7318" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197be89ab4.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">&nbsp;如果，我们把隐藏层的层数增加，我们可以得到Deep Boltzmann Machine(DBM)；如果我们在靠近可视层的部分使用贝叶斯信念网络（即有向图模型，当然这里依然限制层中节点之间没有链接），而在最远离可视层的部分使用Restricted Boltzmann Machine，我们可以得到DeepBelief Net（DBN）。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7317" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197c0203fc.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #0c0c0c">9.4、Deep Belief Networks深信度网络</span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">DBNs是一个概率生成模型，与传统的判别模型的神经网络相对，生成模型是建立一个观察数据和标签之间的联合分布，对P(Observation&#124;Label)和 P(Label&#124;Observation)都做了评估，而判别模型仅仅而已评估了后者，也就是P(Label&#124;Observation)。对于在深度神经网络应用传统的BP算法的时候，DBNs遇到了以下问题：</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #7f7f7f"><em>（1）需要为训练提供一个有标签的样本集；</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #7f7f7f"><em>（2）学习过程较慢；</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #7f7f7f"><em>（3）不适当的参数选择会导致学习收敛于局部最优解。</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><span style="color: #7f7f7f"><em><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7316" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197c1a2824.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">DBNs由多个限制玻尔兹曼机（Restricted Boltzmann Machines）层组成，一个典型的神经网络类型如图三所示。这些网络被&ldquo;限制&rdquo;为一个可视层和一个隐层，层间存在连接，但层内的单元间不存在连接。隐层单元被训练去捕捉在可视层表现出来的高阶数据的相关性。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">首先，先不考虑最顶构成一个联想记忆（associative memory）的两层，一个DBN的连接是通过自顶向下的生成权值来指导确定的，RBMs就像一个建筑块一样，相比传统和深度分层的sigmoid信念网络，它能易于连接权值的学习。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">最开始的时候，通过一个非监督贪婪逐层方法去预训练获得生成模型的权值，非监督贪婪逐层方法被Hinton证明是有效的，并被其称为对比分歧（contrastive divergence）。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">在这个训练阶段，在可视层会产生一个向量v，通过它将值传递到隐层。反过来，可视层的输入会被随机的选择，以尝试去重构原始的输入信号。最后，这些新的可视的神经元激活单元将前向传递重构隐层激活单元，获得h（在训练过程中，首先将可视向量值映射给隐单元；然后可视单元由隐层单元重建；这些新可视单元再次映射给隐单元，这样就获取新的隐单元。执行这种反复步骤叫做吉布斯采样）。这些后退和前进的步骤就是我们熟悉的Gibbs采样，而隐层激活单元和可视层输入之间的相关性差别就作为权值更新的主要依据。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">训练时间会显著的减少，因为只需要单个步骤就可以接近最大似然学习。增加进网络的每一层都会改进训练数据的对数概率，我们可以理解为越来越接近能量的真实表达。这个有意义的拓展，和无标签数据的使用，是任何一个深度学习应用的决定性的因素。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px; text-align: center"><img style="border: medium none; max-width: 100%; height: auto" src="read-1.php-7315" tppabs="http://static.leiphone.com/uploads/new/article/740_740/201608/57a197c334a3c.png?imageMogr2/format/jpg/quality/80" border="0" alt="​大牛的《深度学习》笔记，Deep Learning速成教程" /></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">在最高两层，权值被连接到一起，这样更低层的输出将会提供一个参考的线索或者关联给顶层，这样顶层就会将其联系到它的记忆内容。而我们最关心的，最后想得到的就是判别性能，例如分类任务里面。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">在预训练后，DBN可以通过利用带标签数据用BP算法去对判别性能做调整。在这里，一个标签集将被附加到顶层（推广联想记忆），通过一个自下向上的，学习到的识别权值获得一个网络的分类面。这个性能会比单纯的BP算法训练的网络好。这可以很直观的解释，DBNs的BP算法只需要对权值参数空间进行一个局部的搜索，这相比前向神经网络来说，训练是要快的，而且收敛的时间也少。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">DBNs的灵活性使得它的拓展比较容易。一个拓展就是卷积DBNs（Convolutional Deep Belief Networks(CDBNs)）。DBNs并没有考虑到图像的2维结构信息，因为输入是简单的从一个图像矩阵一维向量化的。而CDBNs就是考虑到了这个问题，它利用邻域像素的空域关系，通过一个称为卷积RBMs的模型区达到生成模型的变换不变性，而且可以容易得变换到高维图像。DBNs并没有明确地处理对观察变量的时间联系的学习上，虽然目前已经有这方面的研究，例如堆叠时间RBMs，以此为推广，有序列学习的dubbed temporal convolutionmachines，这种序列学习的应用，给语音信号处理问题带来了一个让人激动的未来研究方向。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">目前，和DBNs有关的研究包括堆叠自动编码器，它是通过用堆叠自动编码器来替换传统DBNs里面的RBMs。这就使得可以通过同样的规则来训练产生深度多层神经网络架构，但它缺少层的参数化的严格要求。与DBNs不同，自动编码器使用判别模型，这样这个结构就很难采样输入采样空间，这就使得网络更难捕捉它的内部表达。但是，降噪自动编码器却能很好的避免这个问题，并且比传统的DBNs更优。它通过在训练过程添加随机的污染并堆叠产生场泛化性能。训练单一的降噪自动编码器的过程和RBMs训练生成模型的过程一样。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="font-size: 24px; color: #ff0000">&#124;</span><span style="color: #000000">十、总结与展望</span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="color: #3f3f3f">1）Deep learning总结</span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">深度学习是关于自动学习要建模的数据的潜在（隐含）分布的多层（复杂）表达的算法。换句话来说，深度学习算法自动的提取分类需要的低层次或者高层次特征。高层次特征，一是指该特征可以分级（层次）地依赖其他特征，例如：对于机器视觉，深度学习算法从原始图像去学习得到它的一个低层次表达，例如边缘检测器，小波滤波器等，然后在这些低层次表达的基础上再建立表达，例如这些低层次表达的线性或者非线性组合，然后重复这个过程，最后得到一个高层次的表达。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">Deep learning能够得到更好地表示数据的feature，同时由于模型的层次、参数很多，capacity足够，因此，模型有能力表示大规模数据，所以对于图像、语音这种特征不明显（需要手工设计且很多没有直观物理含义）的问题，能够在大规模训练数据上取得更好的效果。此外，从模式识别特征和分类器的角度，deep learning框架将feature和分类器结合到一个框架中，用数据去学习feature，在使用中减少了手工设计feature的巨大工作量（这是目前工业界工程师付出努力最多的方面），因此，不仅仅效果可以更好，而且，使用起来也有很多方便之处，因此，是十分值得关注的一套框架，每个做ML的人都应该关注了解一下。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">当然，deep learning本身也不是完美的，也不是解决世间任何ML问题的利器，不应该被放大到一个无所不能的程度。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #3f3f3f"><strong>2）Deep learning未来</strong></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">深度学习目前仍有大量工作需要研究。目前的关注点还是从机器学习的领域借鉴一些可以在深度学习使用的方法特别是降维领域。例如：目前一个工作就是稀疏编码，通过压缩感知理论对高维数据进行降维，使得非常少的元素的向量就可以精确的代表原来的高维信号。另一个例子就是半监督流行学习，通过测量训练样本的相似性，将高维数据的这种相似性投影到低维空间。另外一个比较鼓舞人心的方向就是evolutionary programming approaches（遗传编程方法），它可以通过最小化工程能量去进行概念性自适应学习和改变核心架构。</p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><strong><span style="color: #3f3f3f">Deep learning还有很多核心的问题需要解决：</span></strong></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #3f3f3f"><em>（1）对于一个特定的框架，对于多少维的输入它可以表现得较优（如果是图像，可能是上百万维）？</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #3f3f3f"><em>（2）对捕捉短时或者长时间的时间依赖，哪种架构才是有效的？</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #3f3f3f"><em>（3）如何对于一个给定的深度学习架构，融合多种感知的信息？</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #3f3f3f"><em>（4）有什么正确的机理可以去增强一个给定的深度学习架构，以改进其鲁棒性和对扭曲和数据丢失的不变性？</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px"><span style="color: #3f3f3f"><em>（5）模型方面是否有其他更为有效且有理论依据的深度模型学习算法？</em></span></p><p style="margin: 0px 0px 1em; padding: 0px; position: relative; font-size: 16px; color: #5a5a5a; font-family: 'microsoft yahei'; line-height: 28.8px">探索新的特征提取模型是值得深入研究的内容。此外有效的可并行训练算法也是值得研究的一个方向。当前基于最小批处理的随机梯度优化算法很难在多计算机中进行并行训练。通常办法是利用图形处理单元加速学习过程。然而单个机器GPU对大规模数据识别或相似任务数据集并不适用。在深度学习应用拓展方面，如何合理充分利用深度学习在增强传统学习算法的性能仍是目前各领域的研究重点。</p> 
	</div>
	<div class="textbox-bottom">
	</div>
	<div class="tags" style="display: none">  
	</div>
</div>
<div id="commentWrapper" class="comment-wrapper">
	<a name="topreply"></a>
	<div id="addnew"></div>	<div class="comment-pages">
	
	</div>
</div>

					<div class="article-bottom" style="display: none">
						<div class="pages">
							
						</div>
					</div>
				</div>
			</div>
		<div id="sidebar" class="sidebar">
			<div id="innerSidebar">
				<div id="innerSidebarSearch">
					<form method="post" action="http://i.renjihe.com/blog/visit.php">
	<input name="job" type="hidden" value="search"/>
	<input name="keyword" class="search-field" type="text"/>
	<select name="searchmethod"><option value="1">日志标题</option><option value="2">日志全文</option><option value="3">评论引用</option><option value="4">所有留言</option></select>
	<input value="搜索" class="button" type="submit"/>
	</form>
				</div>
				<div id="innerSidebarOne">
					<div id='panelCategory' class="panel">
<h5 style="cursor: pointer" onclick='showhidediv("sideblock_category");'>分类</h5>
<div class="panel-content" id="sideblock_category" style="display: block">
<ul><li><a href="index.php-go=category_1.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_1" title="硬件">硬件</a> [1] <a href="feed.php-go=category_1" tppabs="http://i.renjihe.com/blog/feed.php?go=category_1"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_6.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_6" title="fpga/cpld">fpga/cpld</a> [10] <a href="feed.php-go=category_6" tppabs="http://i.renjihe.com/blog/feed.php?go=category_6"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_66.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_66" title="模电与电路">模电与电路</a> [1] <a href="feed.php-go=category_66" tppabs="http://i.renjihe.com/blog/feed.php?go=category_66"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_7.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_7" title="DSP">DSP</a> [14] <a href="feed.php-go=category_7" tppabs="http://i.renjihe.com/blog/feed.php?go=category_7"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_27.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_27" title="硬件知识">硬件知识</a> [26] <a href="feed.php-go=category_27" tppabs="http://i.renjihe.com/blog/feed.php?go=category_27"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_73.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_73" title="3D打印">3D打印</a> [1] <a href="feed.php-go=category_73" tppabs="http://i.renjihe.com/blog/feed.php?go=category_73"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_64.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_64" title="资讯">资讯</a> [18] <a href="feed.php-go=category_64" tppabs="http://i.renjihe.com/blog/feed.php?go=category_64"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_78.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_78" title="树莓派">树莓派</a> [5] <a href="feed.php-go=category_78" tppabs="http://i.renjihe.com/blog/feed.php?go=category_78"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_2.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_2" title="软件">软件</a> [0] <a href="feed.php-go=category_2.htm" tppabs="http://i.renjihe.com/blog/feed.php?go=category_2"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_9.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_9" title="linux">linux</a> [912] <a href="feed.php-go=category_9" tppabs="http://i.renjihe.com/blog/feed.php?go=category_9"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_14.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_14" title="QT">QT</a> [131] <a href="feed.php-go=category_14" tppabs="http://i.renjihe.com/blog/feed.php?go=category_14"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_82.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_82" title="Lua">Lua</a> [15] <a href="feed.php-go=category_82" tppabs="http://i.renjihe.com/blog/feed.php?go=category_82"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_15.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_15" title="Android">Android</a> [261] <a href="feed.php-go=category_15" tppabs="http://i.renjihe.com/blog/feed.php?go=category_15"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_65.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_65" title="Iphone">Iphone</a> [48] <a href="feed.php-go=category_65" tppabs="http://i.renjihe.com/blog/feed.php?go=category_65"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_32.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_32" title="windows">windows</a> [61] <a href="feed.php-go=category_32" tppabs="http://i.renjihe.com/blog/feed.php?go=category_32"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_31.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_31" title="webkit">webkit</a> [79] <a href="feed.php-go=category_31" tppabs="http://i.renjihe.com/blog/feed.php?go=category_31"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_10.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_10" title="C/C++">C/C++</a> [98] <a href="feed.php-go=category_10" tppabs="http://i.renjihe.com/blog/feed.php?go=category_10"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_29.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_29" title="lisp、Erlang">Erlang、Lisp</a> [28] <a href="feed.php-go=category_29" tppabs="http://i.renjihe.com/blog/feed.php?go=category_29"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_46.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_46" title="Python">Python</a> [255] <a href="feed.php-go=category_46" tppabs="http://i.renjihe.com/blog/feed.php?go=category_46"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_18.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_18" title="Java">Java</a> [25] <a href="feed.php-go=category_18" tppabs="http://i.renjihe.com/blog/feed.php?go=category_18"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_13.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_13" title="web">web</a> [486] <a href="feed.php-go=category_13" tppabs="http://i.renjihe.com/blog/feed.php?go=category_13"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_50.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_50" title="测试">测试</a> [12] <a href="feed.php-go=category_50" tppabs="http://i.renjihe.com/blog/feed.php?go=category_50"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_23.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_23" title="源码分析">源码分析</a> [85] <a href="feed.php-go=category_23" tppabs="http://i.renjihe.com/blog/feed.php?go=category_23"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_19.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_19" title="编译原理">编译原理</a> [82] <a href="feed.php-go=category_19" tppabs="http://i.renjihe.com/blog/feed.php?go=category_19"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_83.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_83" title="黑客">黑客</a> [4] <a href="feed.php-go=category_83" tppabs="http://i.renjihe.com/blog/feed.php?go=category_83"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_39.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_39" title="GPU编程">GPU编程</a> [89] <a href="feed.php-go=category_39" tppabs="http://i.renjihe.com/blog/feed.php?go=category_39"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_48.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_48" title="OpenGL">OpenGL</a> [117] <a href="feed.php-go=category_48" tppabs="http://i.renjihe.com/blog/feed.php?go=category_48"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_52.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_52" title="OpenCV">OpenCV</a> [18] <a href="feed.php-go=category_52" tppabs="http://i.renjihe.com/blog/feed.php?go=category_52"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_20.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_20" title="音视频与流媒体">音视频与流媒体</a> [412] <a href="feed.php-go=category_20" tppabs="http://i.renjihe.com/blog/feed.php?go=category_20"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_62.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_62" title="游戏">游戏</a> [455] <a href="feed.php-go=category_62" tppabs="http://i.renjihe.com/blog/feed.php?go=category_62"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_54.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_54" title="FLEX&AS">FLEX&amp;AS</a> [124] <a href="feed.php-go=category_54" tppabs="http://i.renjihe.com/blog/feed.php?go=category_54"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_61.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_61" title="编程杂项">编程杂项</a> [55] <a href="feed.php-go=category_61" tppabs="http://i.renjihe.com/blog/feed.php?go=category_61"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_57.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_57" title="网络安全">网络安全</a> [43] <a href="feed.php-go=category_57" tppabs="http://i.renjihe.com/blog/feed.php?go=category_57"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_28.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_28" title="软件工程">软件工程</a> [40] <a href="feed.php-go=category_28" tppabs="http://i.renjihe.com/blog/feed.php?go=category_28"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_74.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_74" title="并行编程">并行编程</a> [4] <a href="feed.php-go=category_74" tppabs="http://i.renjihe.com/blog/feed.php?go=category_74"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_77.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_77" title="大数据">大数据</a> [10] <a href="feed.php-go=category_77" tppabs="http://i.renjihe.com/blog/feed.php?go=category_77"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_37.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_37" title="理论">理论</a> [0] <a href="feed.php-go=category_37.htm" tppabs="http://i.renjihe.com/blog/feed.php?go=category_37"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_38.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_38" title="数学">数学</a> [75] <a href="feed.php-go=category_38" tppabs="http://i.renjihe.com/blog/feed.php?go=category_38"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_43.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_43" title="物理">物理</a> [19] <a href="feed.php-go=category_43" tppabs="http://i.renjihe.com/blog/feed.php?go=category_43"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_24.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_24" title="算法">算法</a> [80] <a href="feed.php-go=category_24" tppabs="http://i.renjihe.com/blog/feed.php?go=category_24"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_40.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_40" title="音频处理">音频处理</a> [3] <a href="feed.php-go=category_40" tppabs="http://i.renjihe.com/blog/feed.php?go=category_40"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_36.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_36" title="CV&Graphic">CV&amp;Graphic</a> [183] <a href="feed.php-go=category_36" tppabs="http://i.renjihe.com/blog/feed.php?go=category_36"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li class="indent"><a href="index.php-go=category_35.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_35" title="AR&AI&模式识别">AR&amp;AI&amp;模式识别</a> [127] <a href="feed.php-go=category_35" tppabs="http://i.renjihe.com/blog/feed.php?go=category_35"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_26.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_26" title="工具">工具</a> [56] <a href="feed.php-go=category_26" tppabs="http://i.renjihe.com/blog/feed.php?go=category_26"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_3.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_3" title="新鲜玩意">新鲜玩意</a> [446] <a href="feed.php-go=category_3" tppabs="http://i.renjihe.com/blog/feed.php?go=category_3"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_63.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_63" title="行业应用">行业应用</a> [6] <a href="feed.php-go=category_63" tppabs="http://i.renjihe.com/blog/feed.php?go=category_63"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_4.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_4" title="资讯">资讯</a> [137] <a href="feed.php-go=category_4" tppabs="http://i.renjihe.com/blog/feed.php?go=category_4"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_12.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_12" title="网文">网文</a> [622] <a href="feed.php-go=category_12" tppabs="http://i.renjihe.com/blog/feed.php?go=category_12"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_41.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_41" title="点子正传">点子正传</a> [20] <a href="feed.php-go=category_41" tppabs="http://i.renjihe.com/blog/feed.php?go=category_41"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_56.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_56" title="理财">理财</a> [24] <a href="feed.php-go=category_56" tppabs="http://i.renjihe.com/blog/feed.php?go=category_56"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_55.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_55" title="收集">收集</a> [157] <a href="feed.php-go=category_55" tppabs="http://i.renjihe.com/blog/feed.php?go=category_55"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_71.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_71" title="创业">创业</a> [405] <a href="feed.php-go=category_71" tppabs="http://i.renjihe.com/blog/feed.php?go=category_71"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_80.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_80" title="共享软件">共享软件</a> [12] <a href="feed.php-go=category_80" tppabs="http://i.renjihe.com/blog/feed.php?go=category_80"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_72.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_72" title="创意">创意</a> [20] <a href="feed.php-go=category_72" tppabs="http://i.renjihe.com/blog/feed.php?go=category_72"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_85.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_85" title="星象命理">星象命理</a> [40] <a href="feed.php-go=category_85" tppabs="http://i.renjihe.com/blog/feed.php?go=category_85"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_11.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_11" title="杂项">杂项</a> [50] <a href="feed.php-go=category_11" tppabs="http://i.renjihe.com/blog/feed.php?go=category_11"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_58.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_58" title="记录">记录</a> [26] <a href="feed.php-go=category_58" tppabs="http://i.renjihe.com/blog/feed.php?go=category_58"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_79.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_79" title="心灵鸡汤">心灵鸡汤</a> [78] <a href="feed.php-go=category_79" tppabs="http://i.renjihe.com/blog/feed.php?go=category_79"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_75.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_75" title="文学">文学</a> [9] <a href="feed.php-go=category_75" tppabs="http://i.renjihe.com/blog/feed.php?go=category_75"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_67.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_67" title="其它工程资料">其它工程资料</a> [2] <a href="feed.php-go=category_67" tppabs="http://i.renjihe.com/blog/feed.php?go=category_67"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_84.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_84" title="哲学">哲学</a> [24] <a href="feed.php-go=category_84" tppabs="http://i.renjihe.com/blog/feed.php?go=category_84"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_87.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_87" title="有言">有言</a> [9] <a href="feed.php-go=category_87" tppabs="http://i.renjihe.com/blog/feed.php?go=category_87"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_88.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_88" title="微语录">微语录</a> [15] <a href="feed.php-go=category_88" tppabs="http://i.renjihe.com/blog/feed.php?go=category_88"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_76.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_76" title="置顶">置顶</a> [440] <a href="feed.php-go=category_76" tppabs="http://i.renjihe.com/blog/feed.php?go=category_76"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_86.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_86" title="励志">励志</a> [10] <a href="feed.php-go=category_86" tppabs="http://i.renjihe.com/blog/feed.php?go=category_86"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li><li><a href="index.php-go=category_81.htm" tppabs="http://i.renjihe.com/blog/index.php?go=category_81" title="资源管理">资源管理</a> [1] <a href="feed.php-go=category_81" tppabs="http://i.renjihe.com/blog/feed.php?go=category_81"><img src="rss.png" tppabs="http://i.renjihe.com/blog/template/default/images/rss.png" border="0" alt="RSS" title="追踪这个分类的RSS" /></a></li></ul>
</div>
</div><div class="panel">
<h5 onclick='showhidediv("sidebar_calendar");'>日历</h5>
<div class="panel-content" id="sidebar_calendar" style="display: block">
<table id="calendar" cellspacing="1" width="100%">
<tbody><tr><td colspan="7" class="calendar-top">
<a href="index.php-go=archive&cm=9&cy=2015.htm" tppabs="http://i.renjihe.com/blog/index.php?go=archive&cm=9&cy=2015" rel="noindex,nofollow">&lt;</a>
<a href="archive.php.htm" tppabs="http://i.renjihe.com/blog/archive.php" rel="noindex,nofollow"><span class="calendar-year">2016</span></a>
<a href="index.php-go=archive&cm=9&cy=2017.htm" tppabs="http://i.renjihe.com/blog/index.php?go=archive&cm=9&cy=2017" rel="noindex,nofollow">&gt;</a>
	&nbsp;&nbsp;
<a href="index.php-go=archive&cm=8&cy=2016.htm" tppabs="http://i.renjihe.com/blog/index.php?go=archive&cm=8&cy=2016" rel="noindex,nofollow">&lt;</a>
<a href="index.php-go=archive&cm=9&cy=2016.htm" tppabs="http://i.renjihe.com/blog/index.php?go=archive&cm=9&cy=2016" rel="noindex,nofollow"><span class="calendar-month">9</span></a>
<a href="index.php-go=archive&cm=10&cy=2016.htm" tppabs="http://i.renjihe.com/blog/index.php?go=archive&cm=10&cy=2016" rel="noindex,nofollow">&gt;</a>
</td></tr>
<tr class="calendar-weekdays">
	<td class="calendar-weekday-cell">日</td>
	<td class="calendar-weekday-cell">一</td>
	<td class="calendar-weekday-cell">二</td>
	<td class="calendar-weekday-cell">三</td>
	<td class="calendar-weekday-cell">四</td>
	<td class="calendar-weekday-cell">五</td>
	<td class="calendar-weekday-cell">六</td>
</tr>
<tr class="calendar-weekdays"><td class="calendar-sunday"></td><td class="calendar-day"></td><td class="calendar-day"></td><td class="calendar-day"></td><td id="cal1" class="calendar-day"><a href="index.php-go=showday_2016-9-1.htm" tppabs="http://i.renjihe.com/blog/index.php?go=showday_2016-9-1" rel="noindex,nofollow">1</a></td><td id="cal2" class="calendar-day">2</td><td id="cal3" class="calendar-today">3</td></tr><tr class="calendar-weekdays"><td id="cal4" class="calendar-sunday">4</td><td id="cal5" class="calendar-day">5</td><td id="cal6" class="calendar-day">6</td><td id="cal7" class="calendar-day">7</td><td id="cal8" class="calendar-day">8</td><td id="cal9" class="calendar-day">9</td><td id="cal10" class="calendar-saturday">10</td></tr><tr class="calendar-weekdays"><td id="cal11" class="calendar-sunday">11</td><td id="cal12" class="calendar-day">12</td><td id="cal13" class="calendar-day">13</td><td id="cal14" class="calendar-day">14</td><td id="cal15" class="calendar-day">15</td><td id="cal16" class="calendar-day">16</td><td id="cal17" class="calendar-saturday">17</td></tr><tr class="calendar-weekdays"><td id="cal18" class="calendar-sunday">18</td><td id="cal19" class="calendar-day">19</td><td id="cal20" class="calendar-day">20</td><td id="cal21" class="calendar-day">21</td><td id="cal22" class="calendar-day">22</td><td id="cal23" class="calendar-day">23</td><td id="cal24" class="calendar-saturday">24</td></tr><tr class="calendar-weekdays"><td id="cal25" class="calendar-sunday">25</td><td id="cal26" class="calendar-day">26</td><td id="cal27" class="calendar-day">27</td><td id="cal28" class="calendar-day">28</td><td id="cal29" class="calendar-day">29</td><td id="cal30" class="calendar-day">30</td><td class="calendar-saturday"></td></tr>
</tbody></table>
</div>
</div><div class="panel">
<h5 onclick='showhidediv("sidebar_statistics");'>统计</h5>
<div class="panel-content" id="sidebar_statistics" style="display: block">
访问次数 133640<br/>今日访问 1<br/>日志数量 7121<br/><a href="guestbook.php.htm" tppabs="http://i.renjihe.com/blog/guestbook.php">留言数量 2</a><br/>在线人数 1<br/>
</div>
</div><div class="panel">
<h5 onclick='showhidediv("sidebar_entries");'>最新日志</h5>
<div class="panel-content" id="sidebar_entries" style="display: block">
<ul><li class='rowcouple'><a href="read.php-7527.htm" tppabs="http://i.renjihe.com/blog/read.php?7527" title="mysql优化之索引优化">mysql优化之索引优化</a></li><li class='rowodd'><a href="read.php-7526.htm" tppabs="http://i.renjihe.com/blog/read.php?7526" title="Mysql limit 优化，百万至千万级快速分页，--复合索引的引用并应用于轻量级框架&nbsp;&nbsp;">Mysql limit ...</a></li><li class='rowcouple'><a href="read.php-7525.htm" tppabs="http://i.renjihe.com/blog/read.php?7525" title="mysql sql语句执行时间查询">mysql sql语句执...</a></li><li class='rowodd'><a href="read.php-7524.htm" tppabs="http://i.renjihe.com/blog/read.php?7524" title="GeoHash核心原理解析">GeoHash核心原理解...</a></li><li class='rowcouple'><a href="read.php-7523.htm" tppabs="http://i.renjihe.com/blog/read.php?7523" title="如何实现按距离排序、范围查找">如何实现按距离排序、范围...</a></li></ul>
</div>
</div>
				</div>
				<div id="innerSidebarTwo">
					<div class="panel">
<h5 onclick='showhidediv("sidebar_link");'>链接</h5>
<div class="panel-content" id="sidebar_link" style="display: block">
<ul><li><strong>协会</strong></li><li class="indent"><a href="javascript:if(confirm('http://www.siggraph.org/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.siggraph.org/'" tppabs="http://www.siggraph.org/" target="_blank" title="siggraph - ">siggraph</a></li><li class="indent"><a href="javascript:if(confirm('http://www.siggraph.org.cn/Pages/index.aspx  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.siggraph.org.cn/Pages/index.aspx'" tppabs="http://www.siggraph.org.cn/Pages/index.aspx" target="_blank" title="siggraph中国 - ">siggraph中国</a></li><li class="indent"><a href="javascript:if(confirm('http://www.igf.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.igf.com/'" tppabs="http://www.igf.com/" target="_blank" title="独立游戏节 - ">独立游戏节</a></li><li><strong>项目</strong></li><li class="indent"><a href="javascript:if(confirm('http://youyur.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://youyur.com/'" tppabs="http://youyur.com/" target="_blank" title="有鱼 - ">有鱼</a></li><li class="indent"><a href="javascript:if(confirm('http://codecanyon.net/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://codecanyon.net/'" tppabs="http://codecanyon.net/" target="_blank" title="codecanyon - ">codecanyon</a></li><li class="indent"><a href="javascript:if(confirm('http://www.sxsoft.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.sxsoft.com/'" tppabs="http://www.sxsoft.com/" target="_blank" title="软件项目交易网 - ">软件项目交易网</a></li><li class="indent"><a href="javascript:if(confirm('http://www.csto.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.csto.com/'" tppabs="http://www.csto.com/" target="_blank" title="csto - ">csto</a></li><li class="indent"><a href="javascript:if(confirm('http://themeforest.net/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://themeforest.net/'" tppabs="http://themeforest.net/" target="_blank" title="themeforest - ">themeforest</a></li><li class="indent"><a href="javascript:if(confirm('http://www.digitalriver.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.digitalriver.com/'" tppabs="http://www.digitalriver.com/" target="_blank" title="Digital River - ">Digital River</a></li><li class="indent"><a href="javascript:if(confirm('http://www.vworker.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.vworker.com/'" tppabs="http://www.vworker.com/" target="_blank" title="vworker - ">vworker</a></li><li class="indent"><a href="javascript:if(confirm('http://luexiao.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://luexiao.com/'" tppabs="http://luexiao.com/" target="_blank" title="略晓 - ">略晓</a></li><li class="indent"><a href="javascript:if(confirm('http://www.yateshi.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.yateshi.com/'" tppabs="http://www.yateshi.com/" target="_blank" title="雅特士外包网  - ">雅特士外包网 </a></li><li class="indent"><a href="javascript:if(confirm('http://wpappstore.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://wpappstore.com/'" tppabs="http://wpappstore.com/" target="_blank" title="WP App Store - ">WP App Store</a></li><li class="indent"><a href="javascript:if(confirm('http://store.steampowered.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://store.steampowered.com/'" tppabs="http://store.steampowered.com/" target="_blank" title="Steam - ">Steam</a></li><li class="indent"><a href="javascript:if(confirm('http://www.taskcity.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.taskcity.com/'" tppabs="http://www.taskcity.com/" target="_blank" title="TaskCity - ">TaskCity</a></li><li class="indent"><a href="javascript:if(confirm('http://www.istockphoto.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.istockphoto.com/'" tppabs="http://www.istockphoto.com/" target="_blank" title="iStockphoto - ">iStockphoto</a></li><li class="indent"><a href="javascript:if(confirm('http://envato.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://envato.com/'" tppabs="http://envato.com/" target="_blank" title="Envato - ">Envato</a></li><li class="indent"><a href="javascript:if(confirm('https://tutsplus.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='https://tutsplus.com/'" tppabs="https://tutsplus.com/" target="_blank" title="Tuts+ Premium - ">Tuts+ Premium</a></li><li class="indent"><a href="javascript:if(confirm('http://www.sharebank.com.cn/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.sharebank.com.cn/'" tppabs="http://www.sharebank.com.cn/" target="_blank" title="软行天下  - ">软行天下 </a></li><li class="indent"><a href="javascript:if(confirm('http://www.softreg.com.cn/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.softreg.com.cn/'" tppabs="http://www.softreg.com.cn/" target="_blank" title="中国共享软件注册中心 - ">中国共享软件注册中心</a></li><li class="indent"><a href="javascript:if(confirm('https://www.elance.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='https://www.elance.com/'" tppabs="https://www.elance.com/" target="_blank" title="Elance - ">Elance</a></li><li class="indent"><a href="javascript:if(confirm('http://www.freelancer.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.freelancer.com/'" tppabs="http://www.freelancer.com/" target="_blank" title="freelancer - ">freelancer</a></li><li class="indent"><a href="javascript:if(confirm('http://www.bizsofts.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.bizsofts.com/'" tppabs="http://www.bizsofts.com/" target="_blank" title="软件商务网 - ">软件商务网</a></li><li class="indent"><a href="javascript:if(confirm('https://www.odesk.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='https://www.odesk.com/'" tppabs="https://www.odesk.com/" target="_blank" title="oDesk - ">oDesk</a></li><li class="indent"><a href="javascript:if(confirm('http://store.raspberrypi.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://store.raspberrypi.com/'" tppabs="http://store.raspberrypi.com/" target="_blank" title="树莓派商店 - ">树莓派商店</a></li><li><strong>杂项</strong></li><li class="indent"><a href="javascript:if(confirm('http://www.codinghorror.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.codinghorror.com/'" tppabs="http://www.codinghorror.com/" target="_blank" title="codinghorror - ">codinghorror</a></li><li class="indent"><a href="javascript:if(confirm('http://www.fabiensanglard.net/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.fabiensanglard.net/'" tppabs="http://www.fabiensanglard.net/" target="_blank" title="fabiensanglard - ">fabiensanglard</a></li><li class="indent"><a href="javascript:if(confirm('http://www.intel.com/intelpress/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.intel.com/intelpress/'" tppabs="http://www.intel.com/intelpress/" target="_blank" title="英特尔出版社 - ">英特尔出版社</a></li><li class="indent"><a href="javascript:if(confirm('http://www.opencart.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.opencart.com/'" tppabs="http://www.opencart.com/" target="_blank" title="OpenCart - ">OpenCart</a></li><li class="indent"><a href="javascript:if(confirm('http://www.appinn.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.appinn.com/'" tppabs="http://www.appinn.com/" target="_blank" title="小众软件 - ">小众软件</a></li><li class="indent"><a href="javascript:if(confirm('http://renderhjs.net/shoebox/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://renderhjs.net/shoebox/'" tppabs="http://renderhjs.net/shoebox/" target="_blank" title="shoebox工具 - ">shoebox工具</a></li><li><strong>个人</strong></li><li><strong>阅读</strong></li><li class="indent"><a href="javascript:if(confirm('http://huaban.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://huaban.com/'" tppabs="http://huaban.com/" target="_blank" title="花瓣网 - ">花瓣网</a></li><li class="indent"><a href="javascript:if(confirm('http://www.oralstudy.cn/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.oralstudy.cn/'" tppabs="http://www.oralstudy.cn/" target="_blank" title="英语口语学习网 - ">英语口语学习网</a></li><li class="indent"><a href="javascript:if(confirm('http://www.dspguide.com/pdfbook.htm  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.dspguide.com/pdfbook.htm'" tppabs="http://www.dspguide.com/pdfbook.htm" target="_blank" title="DSP Guide - ">DSP Guide</a></li><li><strong>在线文档</strong></li><li class="indent"><a href="javascript:if(confirm('http://help.adobe.com/zh_CN/flex/mobileapps/index.html  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://help.adobe.com/zh_CN/flex/mobileapps/index.html'" tppabs="http://help.adobe.com/zh_CN/flex/mobileapps/index.html" target="_blank" title="Flex移动开发 - ">Flex移动开发</a></li><li class="indent"><a href="javascript:if(confirm('http://help.adobe.com/zh_CN/as3/learn/index.html  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://help.adobe.com/zh_CN/as3/learn/index.html'" tppabs="http://help.adobe.com/zh_CN/as3/learn/index.html" target="_blank" title="Flex AS3 - ">Flex AS3</a></li><li class="indent"><a href="javascript:if(confirm('http://www.adobe.com/cn/devnet.html  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.adobe.com/cn/devnet.html'" tppabs="http://www.adobe.com/cn/devnet.html" target="_blank" title="Adobe开发者中心 - ">Adobe开发者中心</a></li><li class="indent"><a href="javascript:if(confirm('http://help.adobe.com/zh_CN/FlashPlatform/reference/actionscript/3/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://help.adobe.com/zh_CN/FlashPlatform/reference/actionscript/3/'" tppabs="http://help.adobe.com/zh_CN/FlashPlatform/reference/actionscript/3/" target="_blank" title="ActionScript3.0参考 - ">ActionScript3.0参考</a></li><li class="indent"><a href="javascript:if(confirm('http://help.adobe.com/zh_CN/as3/dev/index.html  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://help.adobe.com/zh_CN/as3/dev/index.html'" tppabs="http://help.adobe.com/zh_CN/as3/dev/index.html" target="_blank" title="ActionScript 3.0 - ">ActionScript 3.0</a></li><li class="indent"><a href="javascript:if(confirm('http://help.adobe.com/zh_CN/air/build/index.html  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://help.adobe.com/zh_CN/air/build/index.html'" tppabs="http://help.adobe.com/zh_CN/air/build/index.html" target="_blank" title="AIR - ">AIR</a></li><li class="indent"><a href="javascript:if(confirm('http://www.ppurl.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.ppurl.com/'" tppabs="http://www.ppurl.com/" target="_blank" title="皮皮书屋 - ">皮皮书屋</a></li><li class="indent"><a href="javascript:if(confirm('http://science.scileaf.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://science.scileaf.com/'" tppabs="http://science.scileaf.com/" target="_blank" title="四叶科学百科 - ">四叶科学百科</a></li><li><strong>临时</strong></li><li class="indent"><a href="javascript:if(confirm('http://www.uisdc.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.uisdc.com/'" tppabs="http://www.uisdc.com/" target="_blank" title="优设 - ">优设</a></li><li class="indent"><a href="javascript:if(confirm('http://www.genesis-3d.com.cn/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.genesis-3d.com.cn/'" tppabs="http://www.genesis-3d.com.cn/" target="_blank" title="genesis-3d - ">genesis-3d</a></li><li class="indent"><a href="javascript:if(confirm('http://designlol.net/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://designlol.net/'" tppabs="http://designlol.net/" target="_blank" title="designlol - ">designlol</a></li></ul>
</div>
</div><div class="panel">
<h5 onclick='showhidediv("sidebar_misc");'>其他</h5>
<div class="panel-content" id="sidebar_misc" style="display: block">
<a href="login.php.htm" tppabs="http://i.renjihe.com/blog/login.php">登入</a><br/><a href="login.php-job=register.htm" tppabs="http://i.renjihe.com/blog/login.php?job=register">注册</a><br/>RSS： <a href="feed.php" tppabs="http://i.renjihe.com/blog/feed.php">日志</a> | <a href="feed.php-go=comment.htm" tppabs="http://i.renjihe.com/blog/feed.php?go=comment">评论</a><br/>编码：UTF-8<br/><a href="javascript:if(confirm('http://validator.w3.org/check?uri=referer  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://validator.w3.org/check?uri=referer'" tppabs="http://validator.w3.org/check?uri=referer" target="_blank">XHTML 1.0</a>
</div>
</div>
				</div>
				<div id="innerSidebarFooter">
				 Powered by <a href="javascript:if(confirm('http://www.bo-blog.com/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.bo-blog.com/'" tppabs="http://www.bo-blog.com/" target="_blank">Bo-Blog 2.1.1 Release</a><span id="footer-security"><a href="javascript:if(confirm('http://www.cnbct.org/  \n\nļ޷ Teleport Ultra , Ϊ һ·ⲿΪʼַĵַ  \n\nڷϴ?'))window.location='http://www.cnbct.org/'" tppabs="http://www.cnbct.org/" target="_blank" title="Code detection by Bug.Center.Team"><img src="detect.gif" tppabs="http://i.renjihe.com/blog/images/others/detect.gif" alt="Code detection by Bug.Center.Team" border="0" /></a></span>
				<div id="processtime">
				</div>
				</div>
			</div>
		</div>
	</div>
		<div id="footer">
		</div>
	</div>
</div>
<script type="text/javascript">
loadSidebar();
</script>
<script type='text/javascript'>
//<![CDATA[
if (document.getElementById('processtime')) document.getElementById('processtime').innerHTML="<span class='runtimedisplay'>Run in 99 ms, 6 Queries.</span>";
//]]>
</script></body>
</html>