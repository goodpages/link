<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
<channel>
<title><![CDATA[流浪的龙－个人知识管理]]></title> 
<link>http://i.renjihe.com/blog/index.php</link> 
<description><![CDATA[]]></description> 
<language>zh-cn</language> 
<copyright><![CDATA[流浪的龙－个人知识管理]]></copyright>
<item>
<link>http://i.renjihe.com/blog/read.php?4395</link>
<title><![CDATA[OpenMP并行编程]]></title> 
<author>renjihe1988 &lt;admin@yourname.com&gt;</author>
<category><![CDATA[并行编程]]></category>
<pubDate>Fri, 01 Feb 2013 06:12:24 +0000</pubDate> 
<guid>http://i.renjihe.com/blog/read.php?4395</guid> 
<description>
<![CDATA[ 
	<p>今天花了点时间针对Openmp共享内存的并行编程写了点东西。这里只能提供一点概念和感觉（甚至有可能会误导。</p><ul><li>什么是OpenMP? <p>&ldquo;<a rel="tag" href="http://vastars.info/tag/openmp" title="标签 OpenMP 下的日志" class="st_tag internal_tag"><span style="color: #585858">OpenMP</span></a> (Open Multi-Processing) is an application programming interface (API) that supports multi-platform shared memory multiprocessing programming in C, C++ and Fortran on many architectures, including Unix and Microsoft Windows platforms. It consists of a set of compiler directives, library routines, and environment variables that influence run-time behavior. &rdquo;</p><p>简单来说，OpenMP是一个可以应用于多种平台的共享内存式并行计算的接口。</p></li><li>Openmp的工作模式： <p>Openmp的工作模式为串行-并行-串行&hellip;。一开始的主线程是串行，当在需要并行的时候（这时候程序中应该有相应的Openmp指令语句），多个线程开始一起工作。若当前的并行块结束（仍旧由相应的Opemp指令语句来控制）时，又重新回到单一的主线程。如此可往复继续。在一个四核心cpu上运行Openmp程序（并行块的线程数默认是核心数目，这里即为4个线程），程序处于主线程时cpu利用率为100%，但是当程序进入并行块时所有的核心都会参与进来，cpu利用率会达到400%。如果程序的主要运算部分都处于并行区域，则绝大部分时间cpu都处于400%的工作状态，这样便大大提高cpu的利用率。</p><p><img class="insertimage" src="attachment.php?fid=1485" border="0" width="588" height="182" /></p></li><li>Openmp程序的结构： <p>正如上面所说，编写Openmp程序只需要在已有的串行程序上稍加修改即可：在并行开始和结束的地方加上Openmp语句引导并行的开始和结束。这些引导语句本身处于注释语句的地位，必须在编译时加上Openmp并行参数才能使其生效。如果不加编译参数，编译出来的程序仍旧是串行程序。</p><p>Openmp是最容易实现的并行方式。</p></li></ul><ul><li>Openmp程序的编写： <p>下面以fortran语言为例说明Openmp程序的编写（对c语言和fortran语言，都可以参考本文最后给出的openmp教程）。一般的格式为</p><p>!$omp <a rel="tag" href="http://vastars.info/tag/parallel" title="标签 parallel 下的日志" class="st_tag internal_tag"><span style="color: #585858">parallel</span></a> <span style="color: #ff0000"><span style="background-color: #fcfcfc">CLAUSE</span></span>!$omp <span style="color: #0000ff"><span style="background-color: #fcfcfc">DIRECTION</span></span>[ structured block of code ]</p><p>!$omp end <span style="background-color: #fcfcfc"><span style="color: #0000ff">DIRECTION</span></span>!$omp end parallel</p><p>其中<span style="color: #0000ff"><span style="background-color: #fcfcfc">DIRECTION</span></span>是Openmp指令，有sections，do等，指定并行行为。中间的</p><p>[ structured block of code ]</p><p>即是需要并线的程序块。除了上面的称为<span style="color: #0000ff"><span style="background-color: #fcfcfc">DIRECTION</span></span>的指令语句外，Openmp还需要称为<span style="color: #ff0000"><span style="background-color: #fcfcfc">CLAUSE</span></span>的从句对并行进行限制和说明。比如，需要对私有变量进行声明时就需要用到private从句（这是经常要遇到的，后面会以例子说明）。在fortran的串行编译下，以&ldquo;！&rdquo;打头的都处于屏蔽状态是不起作用的。加了openmp编译参数后才会生效。</p><p>我在程序编写中用到最多的是do指令，偶尔用一下sections。do指令通常用来并行化do循环。本来用一个线程来执行的长的do循环被分割成几个部分让多个线程同时执行，这样就节省了时间。sections指令通常用来将前后没有依赖关系的程序块（也即原本不分先后，你换下顺序也无所谓）并行化。因为无关联，所以可以同时执行。</p><p>可以说，若程序主要用来做计算，掌握了do和sections这两个指令足矣！</p></li><li>简单的程序例子： <p>1.Sections 指令的应用：</p><p>!$OMP PARALLEL SHARED(A,B,C), PRIVATE(I) &nbsp; !//Paralell块开始</p><p>!$OMP SECTIONS &nbsp;!//Sections开始</p><p>!$OMP SECTION &nbsp; &nbsp; !//第一个section</p><p>DO I = 1, N/2</p><p>C(I) = A(I) + B(I)</p><p>END DO</p><p>!$OMP SECTION &nbsp; &nbsp;!//第二个section</p><p>DO I = 1+N/2, N</p><p>C(I) = A(I) + B(I)</p><p>END DO</p><p>!$OMP END SECTIONS NOWAIT &nbsp; &nbsp; &nbsp; !//Sections结束</p><p>!$OMP END PARALLEL &nbsp; &nbsp;&nbsp;!//Paralell块结束</p><p>这个并行语句将本来从1到N的循环手动分为两个部分并行执行。上面的shared,private就是从句（clause），声明A,B,C为公有的，而循环指标I是私有的。因为两个section同时执行，都会对I进行改变，所以两个section的循环指标必须彼此独立，不能是同一个变量。PRIVATE会自动将这个会引发冲突的变量按需生成多个拷贝以供使用。最后的!$OMP END SECTIONS NOWAIT语句告诉两个线程可各自自行结束，无需相互等待。</p><p>2.Do 指令的应用：</p><p>上面用Section实现的功能完全可以用Do来实现：</p><p>!$OMP PARALLEL SHARED(A,B,C), PRIVATE(I) &nbsp; &nbsp;!//Paralell块开始</p><p>!$OMP DO &nbsp; &nbsp;!//Do的并行开始</p><p>DO I = 1, N</p><p>C(I) = A(I) + B(I)</p><p>END DO</p><p>!$OMP END DO &nbsp; !//Do的并行结束</p><p>!$OMP END PARALLEL &nbsp;&nbsp;!//Paralell块结束</p><p>Do循环本来是从1到N，现在有多少个线程就分为多少个部分执行，比上面的section更方便智能。不用担心循环次数N不能被线程数整除~。一般情况下，各个线程均分循环次数，但是在某些循环指标下运算可能比较快，所以各个线程的运算时间可能不尽相同。这时候如果需要让各个线程都结束了才能再往下（没有NOWAIT），快的线程就必须等待慢的线程。为了解决这个问题需要加上schedule从句，首行变为如下：</p><p>!$OMP PARALLEL SHARED(A,B,C), PRIVATE(I),<span style="color: #ff0000"><span style="background-color: #fcfcfc">SCHEDULE(DYNAMIC)</span></span></p><p>这个SCHEDULE(DYNAMIC)从句告诉程序动态调整并线方式，那些任务轻松运算快的线程会自动去帮任务重运算慢的线程，力争所有线程同时完成任务。</p><p>关于Do的积累计算，如累加，需要加上REDUCTION从句：</p><p>C=0.d0</p><p>!$OMP PARALLEL SHARED(A,C), PRIVATE(I),<span style="color: #ff0000"><span style="background-color: #fcfcfc">REDUCTION(+:C)</span></span>!$OMP DO</p><p>DO I = 1, N</p><p>C =C+ A(I)</p><p>END DO</p><p>!$OMP END DO</p><p>!$OMP END PARALLEL</p><p>这里将累加分为几个部分由多个线程进行运算，由于各个线程都在0.d0的基础上开始计算它该算的部分，所以最后必须将各部分计算的结果再次求和。<span style="color: #ff0000"><span style="background-color: #fcfcfc">REDUCTION(+:C)</span></span>从句就实现了这个效果。类似的叠乘等等用类似写法，只需把&ldquo;：&rdquo;前的运算符改为乘法&ldquo;*&rdquo;即可。</p></li><li>一些注意问题： <p>1.尤其要注意的问题就是变量的私有和公有问题。其实只要把握好一个原则，即如果这个变量有可能会被不同的线程同时进行写操作（这不是你希望看到的），则这个变量就应该声明为私有。一般来说，并行体中临时用到的一些中间变量应该是私有的。</p><p>2.据我的经验，Fortran中如果不特别声明，变量都是默认公有的。这一点可以用DEFAULT(PRIVATE/SHARED)从句强行改变。循环指标默认是私有的，无需自己另外声明。放在common域中的变量都是全局的，若要将这些全局变量私有化，可使用threadprivate指令（参见文章：<span style="line-height: 18px; color: #666666"><a style="padding-bottom: 0px; margin: 0px; padding-left: 0px; padding-right: 0px; color: #2277dd; text-decoration: none; padding-top: 0px" href="http://vastars.info/parallelcomputing/threadprivate.html" target="_blank" title="OpenMP并行编程：threadprivate指令"><span style="background-color: #fcfcfc">OpenMP并行编程：threadprivate指令</span></a></span>）。</p><p>3.并行引导语句可以简化，但要注意前后配对。比如上面那个累加的例子可以这样写：</p><p>C=0.d0</p><p>!$OMP PARALLEL DO SHARED(A,C), PRIVATE(I),<span style="color: #ff0000"><span style="background-color: #fcfcfc">REDUCTION(+:C)</span></span>DO I = 1, N</p><p>C =C+ A(I)</p><p>END DO</p><p>!$OMP END PARALLEL DO</p><p>也即可以将从句加在指令之后。</p><p>4.Fortran+Openmp的编译问题：</p><p>一般来说，加上-openmp编译参数即可。如：</p><p>ifort -openmp -o exe.out main.f</p><p>gfortran用-fopenmp编译参数，g77和ifort一样用-openmp参数。</p><p>如果用Makefile，将编译参数放在合适的地方。</p><p>5.对于多重do循环，如果中间变量太多，对私有公有弄不清楚或者虽然清楚但是闲麻烦，可以保留最外层循环，将里面的循环在别处写成一个子函数或子程序 ，然后在此处调用。这样从结构上看就是对一重循环进行并行化，条理清楚不容易出错。当然，传递给子函数或子程序的参数一般是要声明私有的。</p><p>6.将串行程序改为Openmp并行程序后，在加与不加-openmp编译参数的情况下分别编译并运算，比较并行与串行的结果，确保并行块没有改错。</p><p>7.可以在并行开始前指定由多少个线程来并行。在单cpu单核心的机器上也可以（虽然没有实际意义，但可以用来调试并行程序）：</p><p></p><p>CALL OMP_SET_NUM_THREADS(scalar_integer_expression)</p><p>其中scalar_integer_expression是个整形变量，指定并行的线程数目。</p><p>8.Openmp对私有变量的大小有限制。所以当遇到这样的情况，一般就是由这个限制造成的：不加openmp并行时程序没有问题，加了openmp并行时出现断错误（segmentation fault），但是当把某个（一些）私有数组的维数变小时，段错误消失而且和串行时结果一致。</p><p>解决办法（linux下，windows下另外search）如下：</p><p>在linux终端执行</p><p>ulimit -s unlimited ;export KMP_STACKSIZE=<span style="background-color: #fcfcfc"><span><span style="font-family: Arial, sans-serif">2048000</span><span style="line-height: 16px; border-collapse: collapse; font-family: Arial, sans-serif; color: #000000; font-size: 12px; font-weight: 200"><em> </em></span></span></span></p><p>后一个数字参数足够大即可。</p></li></ul><p>原文来源：<a href="http://vastars.info/parallelcomputing/openmp.html">http://vastars.info/parallelcomputing/openmp.html</a></p>
]]>
</description>
</item><item>
<link>http://i.renjihe.com/blog/read.php?4394</link>
<title><![CDATA[关于异构计算]]></title> 
<author>renjihe1988 &lt;admin@yourname.com&gt;</author>
<category><![CDATA[并行编程]]></category>
<pubDate>Fri, 01 Feb 2013 06:11:40 +0000</pubDate> 
<guid>http://i.renjihe.com/blog/read.php?4394</guid> 
<description>
<![CDATA[ 
	<div class="con news_content"><p><strong>一、关于异构计算：</strong></p><p>&nbsp;&nbsp;&nbsp; <strong>什么是&ldquo;异构系统架构&rdquo;（HSA）？它是否要代替FSA？</strong></p><p>&nbsp;&nbsp;&nbsp; <strong>A:</strong>&ldquo;异质系统架构&rdquo;（HSA）使软件开发商能够通过将CPU上的串行处理与&ldquo;图形处理器&rdquo;（GPU）上的并行处理结合起来，针对APU轻松编程，同时都还能够以低功耗高带宽访问内存。AMD正在积极努力使HAS成为开发者群体的一个开放的行业标准。</p><p style="text-align: center"></p><p>&nbsp;&nbsp;&nbsp; <strong>为什么AMD认为能够推动HSA成为一个被广泛采用的行业标准？</strong></p><p>&nbsp;&nbsp;&nbsp; <strong>A：</strong>由于2011年APU产量的快速增长，我们预计将看到HSA（它是APU的未来）会很快成为一个行业标准，与此同时AMD将会推动开放的HSA标准。异构计算是以现代应用所要求的低功耗水平实现计算效率的提升所必需的。开放标准能够迅速将整个一个生态系统聚集到一起，使每个人都能在一个公平环境下进行竞争。开放标准会随时间的推移被人们接受，使软件开发人员能够在来自多个硬件厂商的多种平台上运行他们的应用系统。AMD曾成功推动AMD64、HyperTransport 等开放标准以及今天快速增长的OpenCL 生态系统成为开放标准。AMD不仅是推动开放标准，而且还帮助推动并行计算编程工作。</p><p><strong>二、基于异构计算的案例介绍：</strong></p><ul><li><strong>视频防抖：</strong> </li></ul><p>&nbsp;&nbsp;&nbsp; 这项AMD独家拥有的基于 &ldquo;AMD加速并行处理技术 &rdquo;的技术，设计用来在家庭视频播放过程中消除颤动和抖动。8 在去年6月成功推出之后，下一代AMD 视频防抖现在将可通过一个轻型网络浏览器插件来交付，让用户能够在最常用的浏览器上观看流视频，也能通过此插件观看在本地存储的视频。</p><p>&nbsp;&nbsp;&nbsp; 用户可以通过 AMD Catalyst Control Center&trade; 或者&ldquo;VISION 引擎控制中心&rdquo;应用来启用该技术。AMD&ldquo;稳定视频&rdquo;将能够对可在Adobe&reg; Flash&reg; Player 10.2（及以后版本）上运行、或在已被编程使用AMD的解码加速（DXVA）引擎的任何播放器上运行的内容进行操作。AMD&ldquo;视频防抖&rdquo; 并不用于 （a）隔离覆盖内容，标志或字幕， 也不用于（b）改善框体内容、促销/广告或者交错式内容的播放。</p><ul><li><strong>人机交互：</strong> </li></ul><p>&nbsp;&nbsp;&nbsp; 在12日大会正式开始之前，11号下午进行了几场小范围的技术讲解，包括下一代电脑交互界面的讲座。演讲者eyeSight公司人员，他介绍了关于显示器与人之间全新界面的一些技术。核心应用是通过电脑、手机等设备的摄像头来感应使用者，使用者通过手势操作来实现鼠标的功能。EyeSight公司人员表示，未来的人机界面会更加亲和，易用性与互动性更强，通过触摸来实现操作。显示界面更加轻薄化与小型化，甚至可以穿戴。这些是形式上的，而在应用角度则会突出更强的性能可交互性。比如给一个建筑物拍摄一张照片，就可以在照片上出现相关信息，把现实与虚拟结合起来。不需要走进这个建筑，就可以了解到其内部结构，这些都可以从显示界面上表现出来。</p><p>&nbsp;&nbsp;&nbsp; 除了触摸以外，还可以通过肢体动作来对设备进行控制。通过摄像头人脸识别功能找到用户，定义出其手指的位置，这样只要挥手，就可以实现对显示设备上的操作了。手指的移动可以像鼠标定位一样准确。移动手指就是移动鼠标，手指向前点击空气就等于点击鼠标。而且可以自己定义手势动作，实现前进、后退等功能。</p><p>&nbsp;&nbsp;&nbsp; 这些不仅仅对于电脑，在手持设备（手机和平板电脑）上也是如此，只要挥挥手就可以进行控制。所有的这些应用，对于CPU的运算是很大的考验。为了保证快速的响应与识别，一颗强劲的&ldquo;芯&rdquo;是必须的。</p><ul><li><strong>星云计算：</strong> </li></ul><p>&nbsp;&nbsp;&nbsp; &nbsp;&ldquo;GPU的运算性能在大规模数据运算中的重要性&rdquo;</p><p>&nbsp;&nbsp;&nbsp; 演示者通过天文运算这样的大规模数据运算来阐述道理。他展示了一个天文学方面的Demo。星图由很多很多个小图组成，在放大或缩小的过程中要做到无疑切换，需要在图形显示端做到预处理，这必须倚仗于GPU的图形加速性能。使宇宙星图可以无级放大，这必须将无数个小单元的图像信息汇总到一起，通过特定的算法展现出一层层的变化。GPU在其中的数据运算量非常之大。这仅仅靠CPU是不行的，必须要在图形处理一端具有强大和运算性能。</p><p>&nbsp;&nbsp;&nbsp; 对于模拟宇宙星图，如果只是绘出9大行星的运行轨迹是比较容易的，而画出数百个星体的运动轨迹就没有那么简单了，但还是可行的。通过GPU的加速，可以实现多至数百万个运算个体的的处理工作，运动图像可以达到60帧/秒的变化，让人没有停顿的感觉，真正实现无级变化。当然这些不仅仅靠一颗强劲的CPU就可以完成的，必须在有GPU的运算支持。</p><p>&nbsp;&nbsp;&nbsp; 在数据处理中可能会出现这样的情况，CPU在运算过程中有空闲，而内存与GPU之间的运算非常繁忙。这个时候需要将有时间空闲的部分与无时间空闲的部分结合起来，提高工作效率。将这二者能够很好结合起来的解决方案就是APU，它可以有效降低CPU与GPU工作当中出现的不对等，尽量平均分配工作线程与运算量，做到均衡高效运算。由此不难看出，GPU在专业大数据计算时的重要性，同时对于我们未来的应用，也起着非常重要的作用。&nbsp;</p><ul><li><strong>暴风影音：</strong> </li></ul><p>&nbsp;&nbsp;&nbsp; 目前，视频处理和播放是当前用户很常用的应用。始终致力于视频技术的研究和发展的暴风目前已成为中国市场领先的视频软件提供者。暴风影音多媒体播放器内嵌如视频后期处理、异构计算支持的视频转码多项领先的特性，目前已拥有2.5亿用户。</p><p>&nbsp;&nbsp;&nbsp; 在AFDS上，暴风主讲嘉宾暴风影音董事CTO杨立东以对视频用户及市场的深刻洞察进行了行业分析，他例举的诸多暴风播放器特性，如&ldquo;左眼&rdquo;技术、高清增强和色泽增强，均在其第五代产品（针对AMD进行过优化）中有体现。暴风影音会对于APU进行了专门的优化，可以在播放时得到更好的效果。杨立东表示，这是AMD与暴风影音进行了一系列合作之一，针对于APU产品，基于OPENCL做了优化。不仅仅在显示方面效果更好，而且在CPU的占用率上也有明显下降。使用APU平台运行暴风影音，整体性能比使用英特尔核芯显卡平台效果要好。</p><p>&nbsp;&nbsp;&nbsp; 另外，杨立东还对暴风影音的特色功能&ldquo;左眼模式&rdquo;进行了介绍。使用这一模式后，官方称画面清晰度提升可以达到50%，能够使不是高清的电影，达到接近高清的效果，画面看起来更精彩。记者对左眼模式使用的条件以及硬件要求提出了相关问题。杨立东表示，使用左眼模式，对于屏幕大小没有限制，但对于CPU却有要求，推荐使用APU以获得更好的效果。</p></div>
]]>
</description>
</item><item>
<link>http://i.renjihe.com/blog/read.php?4393</link>
<title><![CDATA[异构计算（CPU + GPU）编程简介]]></title> 
<author>renjihe1988 &lt;admin@yourname.com&gt;</author>
<category><![CDATA[并行编程]]></category>
<pubDate>Fri, 01 Feb 2013 06:11:02 +0000</pubDate> 
<guid>http://i.renjihe.com/blog/read.php?4393</guid> 
<description>
<![CDATA[ 
	<p><strong>1.概念</strong></p><p>所谓异构计算，是指CPU+GPU或者CPU+其它设备（如FPGA等）协同计算。一般我们的程序，是在CPU上计算。但是，当大量的数据需要计算时，CPU显得力不从心。那么，是否可以找寻其它的方法来解决计算速度呢？那就是异构计算。例如可利用CPU（Central Processing Unit）、GPU（Graphic Processing Unit）、甚至APU(Accelerated Processing Units，CPU与GPU的融合)等计算设备的计算能力从而来提高系统的速度。异构系统越来越普遍，对于支持这种环境的计算而言，也正受到越来越多的关注。</p><p><strong>2.异构计算的实现 </strong></p><p>目前异构计算使用最多的是利用GPU来加速。主流GPU都采用了统一架构单元，凭借强大的可编程流处理器阵容，GPU在单精度浮点运算方面将CPU远远甩在身后。英特尔Core i7 965处理器，在默认情况下，它的浮点计算能力只有NVIDIA GeForce GTX 280的1/13，与AMD Radeon HD 4870相比差距就更大。</p><p><strong>3.基于GPU编程</strong></p><p>不同厂商通常仅仅提供对于自己设备编程的实现。对于异构系统一般很难用同种风格的编程语言来实现机构编程，而且将不同的设备作为统一的计算单元来处理的难度也是非常大的。基于GPU编程的，目前主要两大厂商提供：一个是NVidia，其提供的GPU编程为CUDA，目前使用的CUDA SDK 4.2.另一个是AMD，其提供的GPU编程为AMD APP（其前身是ATI Stream），目前最新版本AMD APP 2.7。这两个东西是不兼容的，各自为政。作为软件开发者而言，用CUDA开发的软件只能在NVidia相应的显卡上运行，用AMD APP开发的软件，只能在ATI相应的显卡上运行。</p><p><strong>4.OpenCL简介</strong></p><p>那么有没有可能让他们统一起来，简化编程呢？有，那就是由苹果公司发起并最后被业界认可的OpenCL，目前版本1.2。</p><p>开放式计算语言（Open Computing Language:OpenCL），旨在满足这一重要需求。通过定义一套机制，来实现硬件独立的软件开发环境。利用OpenCL可以充分利用设备的并行特性，支持不同级别的并行，并且能有效映射到由CPU，GPU，FPGA（Field－Programmable Gate Array）和将来出现的设备所组成的同构或异构，单设备或多设备的系统。OpenCL定义了运行时，允许用来管理资源，将不同类型的硬件结合在同种执行环境中，并且很有希望在不久的将来，以更加自然的方式支持动态地平衡计算、功耗和其他资源。</p><p><strong>5. DirectCompute简介</strong></p><p>作为软件行业的老大&mdash;微软在这方面又做了什么呢？微软也没闲着，微软推出DirectCompute，与OpenCL抗衡。DirectCompute集成在DX中，目前版本DX11，其中就包括DirectCompute。由于微软的地位，所以大多数厂商也都支持DirectCompute。</p><p><strong>6.GPU计算模型</strong></p><p>内核是执行模型的核心，能在设备上执行。当一个内核执行之前，需要指定一个N-维的范围（NDRange）。一个NDRange是一个一维、二维或三维的索引空间。 还需要指定全局工作节点的数目，工作组中节点的数目。如图<a href="http://www.cnblogs.com/wangshide/archive/2012/01/07/2315830.html#NDRange">NDRange</a>所示，全局工作节点的范围为&#123;12, 12&#125;，工作组的节点范围为&#123;4, 4&#125;，总共有9个工作组。</p><p style="text-align: left; line-height: 17.65pt; margin: 4.75pt 0cm; background: white" class="MsoNormal" align="left"></p><p><img class="insertimage" src="attachment.php?fid=1484" border="0" width="591" height="391" /><br />如果定义向量为1024维，特别地，我们可以定义全局工作节点为1024，工作组中节点为128，则总共有8个组。定义工作组主要是为有些仅需在组内交换数据的程序提供方便。当然工作节点数目的多少要受到设备的限制。如果一个设备有1024个处理节点，则1024维的向量，每个节点计算一次就能完成。而如果一个设备仅有128个处理节点，那么每个节点需要计算8次。合理设置节点数目，工作组数目能提高程序的并行度。</p><p><strong>7.程序实例</strong></p><p>不论是OpenCL还是DirectCompute，其编程风格都基本差不多，程序是分成两部分的：一部分是在设备上执行的（对于我们，是GPU），另一部分是在主机上运行的（对于我们，是CPU）。在设备上执行的程序或许是你比较关注的。它是OpenCL和DirectCompute产生神奇力量的地方。为了能在设备上执行代码，OpenCL程序员需要写一个特殊的函数（kernel函数）放在专用文件中（.cl），这个函数需要使用OpenCL语言编写。OpenCL语言采用了C语言的一部分加上一些约束、关键字和数据类型。在主机上运行的程序提供了API，所以可以管理你在设备上运行的程序。主机程序可以用C或者C++编写，它控制OpenCL的环境（上下文，指令队列&hellip;）。DirectCompute程序员需要写Shader文件（.hlsl），在这个文件中写函数。Shader文件的格式可以查MSDN。</p><p>在写程序时，先初始化设备，然后编译需要在GPU上运行的程序（运行在GPU上的程序是在应用程序运行时编译的）。然后映射需要在GPU上运行的函数名字，OpenCL调用clEnqueueNDRangeKernel执行kernel函数，DirectCompute调用ID3D11DeviceContext:: Dispatch执行Shader函数。函数是并发执行的。</p><p>运行在GPU上的函数一般都很简单。以求和为例：</p><p>用CPU运算</p><p><span style="color: blue">void </span>vector_add_cpu (<span style="color: blue">const float</span>* fIn1,</p><p><span style="color: blue">const</span> <span style="color: blue">float</span>* fIn2,</p><p><span style="color: blue">float</span>*<span> </span><span style="color: #010001">fOut</span>,</p><p><span style="color: blue">const</span> <span style="color: blue">int</span> iN<span style="color: #010001">um</span>)</p><p>&#123;</p><p><span style="color: blue">for</span> (<span style="color: blue">int</span> <span style="color: #010001">i</span> = 0; <span style="color: #010001">i</span> &lt; iN<span style="color: #010001">um</span>; <span style="color: #010001">i</span>++)</p><p>&#123;</p><p><span style="color: #010001">fOut</span> [<span style="color: #010001">i</span>] = fIn1[<span style="color: #010001">i</span>] + fIn2[<span style="color: #010001">i</span>];</p><p>&#125;</p><p>&#125;</p><p>以下是OPenCL的kernel函数</p><p><span style="color: green">//在GPU上，逻辑就会有一些不同。我们使每个线程计算一个元素的方法来代替cpu程序中的循环计算。每个线程的index与要计算的向量的index相同。</span></p><p><span style="color: blue">__kernel</span><span style="color: blue">void</span> vector_add_gpu (<span style="color: blue">__global</span> <span style="color: blue">const</span> <span style="color: blue">float</span>* fIn1,</p><p><span style="color: blue">__global</span> <span style="color: blue">const</span> <span style="color: blue">float</span>* fIn2,</p><p><span style="color: blue">__global</span> <span style="color: blue">float</span>* fOut,</p><p><span style="color: blue">const</span> <span style="color: blue">int</span> iNum)</p><p>&#123;</p><p><span style="color: green">/* get_global_id(0) </span><span style="color: green">返回正在执行的这个线程的ID。</span></p><p><span style="color: green"></span><span style="color: green">许多线程会在同一时间开始执行同一个kernel，</span></p><p><span style="color: green"></span><span style="color: green">每个线程都会收到一个不同的ID，所以必然会执行一个不同的计算。*/</span></p><p><span style="color: blue">const</span> <span style="color: blue">int</span> idx = <span style="color: blue">get_global_id</span>(0);</p><p><span style="color: green">/* 每个work-item都会检查自己的id是否在向量数组的区间内。</span></p><p><span style="color: green">如果在，work-item就会执行相应的计算。*/</span></p><p><span style="color: blue">if</span> (idx &lt; iNum)</p><p>&#123;</p><p>fOut [idx] = fIn1[idx] + fIn2[idx];</p><p>&#125;</p><p>&#125;</p><p>有一些需要注意的地方：</p><p>1. Kernel关键字定义了一个函数是kernel函数。Kernel函数必须返回void。</p><p>2. Global关键字位于参数前面。它定义了参数内存的存放位置。</p><p>另外，所有kernel都必须写在&ldquo;.cl&rdquo;文件中，&ldquo;.cl&rdquo;文件必须只包含OpenCL代码。</p><p>Shader函数</p><p><span style="color: blue">#define</span>NUM_THREAD 16</p><p>StructuredBuffer &lt;<span style="color: blue">float</span>&gt; fInput1: <span style="color: blue">register</span>(t0 );</p><p>StructuredBuffer &lt;<span style="color: blue">float</span>&gt; fInput2: <span style="color: blue">register</span>(t0 );</p><p>StructuredBuffer &lt;<span style="color: blue">float</span>&gt; fOutput: <span style="color: blue">register</span>(u0 );</p><p>[numthreads(NUM_THREAD, 1, 1)]</p><p><span style="color: blue">void</span>vector_add_gpu( <span style="color: blue">uint3</span> Gid: SV_GroupID,</p><p><span style="color: blue">uint3</span> DTid: SV_DispatchThreadID,</p><p><span style="color: blue">uint3</span> GTid: SV_GroupThreadID,</p><p><span style="color: blue">uint</span> GI: SV_GroupIndex )</p><p>&#123;</p><p>fOutput[DTid.x] =<span> </span>fInput1[DTid.x]<span> </span>+ fInput2[DTid.x] ;</p><p>&#125;</p><p>图像旋转是指把定义的图像绕某一点以逆时针或顺时针方向旋转一定的角度，通常是指绕图像的中心以逆时针方向旋转。假设图像的左上角为(l, t), 右下角为(r, b)，则图像上任意点(x, y) 绕其中心(xcenter, ycenter)逆时针旋转&theta;角度后，新的坐标位置(x',y')的计算公式为：</p><p>x&prime;= (x - xcenter) cos&theta; - (y－ycenter) sin&theta; + xcenter,</p><p>y&prime;= (x - xcenter) sin&theta; + (y－ycenter) cos&theta; + ycenter.</p><p>C代码：</p><p><span style="color: blue">void</span>image_rotate(</p><p><span style="color: blue">unsigned</span> <span style="color: blue">int</span>* iInbuf,</p><p><span style="color: blue">unsigned</span> <span style="color: blue">int</span>* iOutbuf,</p><p><span style="color: blue">int</span> iWidth, <span style="color: blue">int</span> iHeight,</p><p><span style="color: blue">float</span> fSinTheta,</p><p><span style="color: blue">float</span> fCosTheta)</p><p>&#123;</p><p><span style="color: blue">int</span> i, j;</p><p><span style="color: blue">int</span> iXc = iWidth /2;</p><p><span style="color: blue">int</span> iYc = iHeight /2;</p><p><span style="color: blue">for</span>(i = 0; i &lt; iHeight; <span style="color: #010001">i</span>++)</p><p>&#123;</p><p><span style="color: blue">for</span>(j=0; j&lt; iWidth; j++)</p><p>&#123;</p><p><span style="color: blue">int</span> iXpos =<span> </span>(j- iXc)*fCosTheta - (i - iYc) * fSinTheta + iXc;</p><p><span style="color: blue">int</span> iYpos =<span> </span>(j- iXc)*fSinTheta + (i - iYc) * fCosTheta + iYc;</p><p><span style="color: blue">if</span>(iXpos &gt;=0&amp;&amp; iYpos &gt;=0&amp;&amp; iXpos &lt; iWidth &amp;&amp; iYpos &lt; iHeight)</p><p>iOutbuf[iYpos * iWidth + iXpos] = iInbuf[i* iWidth +j];</p><p>&#125;</p><p>&#125;</p><p>&#125;</p><p>CL代码：</p><p><span style="color: blue">__kernel</span><span> </span><span style="color: blue">void</span> image_rotate(</p><p><span style="color: blue">__global</span> <span style="color: blue">int</span> * iInbuf,</p><p><span style="color: blue">__global</span> <span style="color: blue">int</span> * iOutbuf,<span> </span><span style="color: green">//Data in global memory</span></p><p><span style="color: blue">int</span> iWidth ,<span> </span><span style="color: blue">int</span> iHeight,<span> </span><span style="color: green">//Image Dimensions</span></p><p><span style="color: blue">float</span> fSinTheta, <span style="color: blue">float</span> fCosTheta )<span> </span><span style="color: green">//Rotation Parameters</span></p><p>&#123;</p><p><span style="color: blue">const</span> <span style="color: blue">int</span> ix = <span style="color: blue">get_global_id</span>(0);</p><p><span style="color: blue">const</span> <span style="color: blue">int</span> iy = <span style="color: blue">get_global_id</span>(1);</p><p><span style="color: blue">int</span> iXc = iWidth /2;</p><p><span style="color: blue">int</span> iYc = iHeight /2;</p><p><span style="color: blue">int</span> iXpos =<span> </span>( ix- iXc)*fCosTheta - (iy- iYc)*fSinTheta+ iXc;</p><p><span style="color: blue">int</span> iYpos =<span> </span>(ix- iXc)*fSinTheta + ( iy- iYc)*fCosTheta+ iYc;</p><p><span style="color: blue">if</span> ((iXpos &gt;=0) &amp;&amp; (iXpos &lt; iWidth )<span> </span>&amp;&amp; (iYpos &gt;=0) &amp;&amp; (iYpos &lt; iHeight))</p><p>iOutbuf[iYpos * iWidth + iXpos]= iInbuf[iy* iWidth +ix];</p><p>&#125;</p><p>不论是OpenCL还是 DirectCompute，其编程还是有些复杂，特别是对于设备的初始化，以及数据交换，非常麻烦。对于初学者难度相当大。那么有没有更简单的编程方法呢？</p><p><strong>8.C++AMP</strong></p><p>还要提到微软，因为我们基本上都使用微软的东东。微软也不错，推出了C++AMP，这是个开放标准，嵌入到VS2012中（VS2012目前还是预览版），在Win7和Win8系统中才能使用，其使用就简单多了：</p><p><span style="color: blue">#include</span><span style="color: #a31515">&lt;amp.h&gt;</span></p><p><span style="color: blue">using</span><span style="color: blue">namespace</span>concurrency;</p><p><span style="color: blue">const</span><span style="color: blue">int</span> size = 5;</p><p><span style="color: blue">void</span>CppAmpMethod()</p><p>&#123;</p><p><span style="color: blue">int</span> aCPP[] = &#123;1, 2, 3, 4, 5&#125;;</p><p><span style="color: blue">int</span> bCPP[] = &#123;6, 7, 8, 9, 10&#125;;</p><p><span style="color: blue">int</span> sumCPP[size];</p><p><span style="color: green">// Create C++ AMP objects.</span></p><p>array_view&lt;<span style="color: blue">const</span> <span style="color: blue">int</span>, 1&gt; a(size,aCPP);</p><p>array_view&lt;<span style="color: blue">const</span> <span style="color: blue">int</span>, 1&gt; b(size, bCPP);</p><p>array_view&lt;<span style="color: blue">int</span>, 1&gt; sum(size, sumCPP);</p><p>sum.discard_data();</p><p>parallel_for_each(</p><p><span style="color: green">// Define the compute domain, which is the set of threads that are created.</span></p><p>sum.extent,</p><p><span style="color: green">// Define the code to run on each thread on the accelerator.</span></p><p>[=](index&lt;1&gt;idx) restrict(amp)</p><p>&#123;</p><p><span style="color: #010001">sum</span>[<span style="color: #010001">idx</span>] = <span style="color: #010001">a</span>[<span style="color: #010001">idx</span>] + <span style="color: #010001">b</span>[<span style="color: #010001">idx</span>];</p><p>&#125;</p><p>);</p><p>&#125;</p><p>就这么简单，只需要包含一个头文件，使用一个命名空间，包含库文件，一切就OK，在调用时，只是一个函数parallel_for_each。（有点类似OpenMP）。</p><p><strong>9.应用</strong></p><p>MATLAB 2010b中Parallel Computing Toolbox与MATLAB Distributed Computing Server的最新版本可利用NVIDIA的CUDA并行计算架构在NVIDIA计算能力1.3以上的GPU上处理数据，执行GPU加速的MATLAB运算，将用户自己的<span>CUDA Kernel</span>函数集成到<span>MATLAB</span>应用程序当中。另外，通过在台式机上使用<span>Parallel Computing Toolbox</span>以及在计算集群上使用<span>MATLAB Distributed Computing Server</span>来运行多个<span>MATLAB worker</span>程序，从而可在多颗<span>NVIDIA GPU</span>上进行计算。<span>AccelerEyes</span>公司开发的<span>Jacket</span>插件也能够使<span>MATLAB</span>利用<span>GPU</span>进行加速计算。<span>Jacket</span>不仅提供了GPU API（应用程序接口），而且还集成了GPU MEX功能。在一定程度说，Jacket是一个完全对用户透明的系统，能够自动的进行内存分配和自动优化。Jacket使用了一个叫&ldquo;on-the- fly&rdquo;的编译系统，使MATLAB交互式格式的程序能够在GPU上运行。目前，Jacket只是基于NVIDIA的CUDA技术，但能够运行在各主流操作系统上。</p><p>从Photoshop CS4开始，Adobe将GPU通用计算技术引入到自家的产品中来。GPU可提供对图像旋转、缩放和放大平移这些常规浏览功能的加速，还能够实现2D/3D合成，高质量抗锯齿，HDR高动态范围贴图，色彩转换等。而在PhotoshopCS5中，更多的算法和滤镜也开始支持GPU加速。另外，Adobe的其他产品如Adobe After EffectsCS4、Adobe Premiere Pro CS4也开始使用GPU进行加速。这些软件借助的也是NVIDIA的CUDA技术。</p><p>Windows 7的核心组成部分包括了支持GPU通用计算的Directcompute API，为视频处理、动态模拟等应用进行加速。Windows 7借助Directcompute增加了对由GPU支持的高清播放的in-the-box支持，可以流畅观看，同时CPU占用率很低。Internet Explorer 9加入了对Directcompute技术的支持，可以调用GPU对网页中的大计算量元素做加速计算；Excel2010、Powerpoint2010&gt;也开始提供对Directcompute技术的支持。</p><p>比利时安特卫普大学，通用电气医疗集团，西门子医疗，东芝中风研究中心和纽约州立大学水牛城分校的都针对GPU加速CT重建进行了各自的研究，不仅如此，西门子医疗用GPU实现了加速MRI中的GRAPPA自动校准，完成<span>MR</span>重建，快速<span>MRI</span>网格化，随机扩散张量磁共振图像（<span>DT-MRI</span>）连通绘图等算法。其他的一些研究者则把医学成像中非常重要的二维与三维图像中器官分割（如<span>Level Set</span>算法），不同来源图像的配准，重建体积图像的渲染等也移植到<span>GPU</span>上进行计算。</p><p><strong>10.不足</strong></p><p>异构并行计算变得越来越普遍，然而对于现今存在的OpenCL和DirectCompute版本来说，的确还存在很多不足，例如编写内核，需要对问题的并行情况做较为深入的分析，对于内存的管理还是需要程序员来显式地申明、显式地在主存和设备的存储器之间进行移动，还不能完全交给系统自动完成。从这些方面，OpenCL和DirectCompute的确还需加强，要使得人们能高效而又灵活地开发应用，还有很多工作要完成。</p>
]]>
</description>
</item><item>
<link>http://i.renjihe.com/blog/read.php?4392</link>
<title><![CDATA[开启新一片蓝海－－异构计算完全解析 ]]></title> 
<author>renjihe1988 &lt;admin@yourname.com&gt;</author>
<category><![CDATA[并行编程]]></category>
<pubDate>Fri, 01 Feb 2013 06:09:55 +0000</pubDate> 
<guid>http://i.renjihe.com/blog/read.php?4392</guid> 
<description>
<![CDATA[ 
	<div id="article_content" class="article_content"><div class="summary" style="padding-bottom: 5px; background-color: #f7f7f7; list-style-type: none; margin: 0px 0px 1.5em; padding-left: 10px; padding-right: 10px; text-decoration: none; padding-top: 5px"><strong>转载于：<a href="http://www.csdn.net/article/2012-07-23/2807633">http://www.csdn.net/article/2012-07-23/2807633</a></strong></div><div class="summary" style="padding-bottom: 5px; background-color: #f7f7f7; list-style-type: none; margin: 0px 0px 1.5em; padding-left: 10px; padding-right: 10px; text-decoration: none; padding-top: 5px"><strong><br />摘要：</strong>本文为您展现了近年来从并行计算到异构计算的发展历程，介绍了异构编程的开发标准：OpenCL、C++ AMP和Java Aparapi，引领读者步入异构计算的瑰丽殿堂，开启另一片蓝海。</div><div class="con news_content" style="padding-bottom: 0px; list-style-type: none; margin: 0px 0px 30px; padding-left: 0px; padding-right: 0px; text-decoration: none; padding-top: 0px"><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">导读：</span></strong><span style="font-family: 微软雅黑">本文为您展现了近年来异构计算的发展历程，介绍了异构编程的开发标准：OpenCL、C++ AMP和Java Aparapi，引领读者步入异构计算的瑰丽殿堂。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">并行计算：让处理的速度变得更快</span></strong></p><p style="text-align: center; padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><img class="insertimage" src="attachment.php?fid=1482" border="0" width="683" height="372" /></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">相对于串行计算，并行计算可以划分成时间并行和空间并行。时间并行即流水线技术，空间并行使用多个处理器执行并发计算，当前研究的主要是空间的并行问题。以程序和算法设计人员的角度看，并行计算又可分为数据并行和任务并行。数据并行把大的任务化解成若干个相同的子任务，处理起来比任务并行简单。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">空间上的并行导致两类并行机的产生，按照麦克&middot;弗莱因（Michael Flynn）的说法分为单指令流多数据流（SIMD）和多指令流多数据流（MIMD），而常用的串行机也称为单指令流单数据流（SISD）。MIMD类的机器又可分为常见的五类：并行向量处理机（PVP）、对称多处理机（SMP）、大规模并行处理机（MPP）、工作站机群（COW）、分布式共享存储处理机（DSM）。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">从自然哲学层面上来讲：任何最为复杂的事情，都可以被拆分成若干个小问题去解决。这就是当今并行计算的哲学理论依据。然而在当今的双路、四路、八路甚至多路处理器系统中，并行计算的概念早已得到广泛应用。目前业界最为普及的并行计算规范就是OpenMP。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">OpenMP：同构计算最为普及的标准</span></strong></p><p style="text-align: center; padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">OpenMP（Open Multi-Processing）是由OpenMP Architecture Review Board牵头提出的，并已被广泛接受的，用于共享内存并行系统的多线程程序设计的一套指导性注释（Compiler Directive）。OpenMP支持的编程语言包括C语言、C++和Fortran；而支持OpenMP的编译器包括Sun Studio和Intel Compiler，以及开放源码的GCC和Open64编译器。OpenMP提供了对并行算法的高层的抽象描述，程序员通过在源代码中加入专用的pragma来指明自己的意图，由此编译器可以自动将程序进行并行化，并在必要之处加入同步互斥以及通信。当选择忽略这些pragma，或者编译器不支持OpenMP时，程序又可退化为通常的程序（一般为串行），代码仍然可以正常运作，只是不能利用多线程来加速程序执行。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">OpenMP的特色。</span></strong><span style="font-family: 微软雅黑">OpenMP提供的这种对于并行描述的高层抽象降低了并行编程的难度和复杂度，这样程序员可以把更多的精力投入到并行算法本身，而非其具体实现细节。对基于数据分集的多线程程序设计，OpenMP是一个很好的选择。同时，使用OpenMP也提供了更强的灵活性，可以较容易的适应不同的并行系统配置。线程粒度和负载平衡等是传统多线程程序设计中的难题，但在OpenMP中，OpenMP库从程序员手中接管了部分这两方面的工作。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">OpenMP的缺点。</span></strong><span style="font-family: 微软雅黑">作为高层抽象，OpenMP并不适合需要复杂的线程间同步和互斥的场合。OpenMP的另一个缺点是不能在非共享内存系统（如计算机集群）上使用。由此如果我们想将不同类型的计算器、计算机联和起来，协同工作。我们就需要使用异构计算技术。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">双剑岂可合璧：什么是异构计算？</span></strong></p><p style="text-align: center; padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">异构计算（Heterogeneous computing）主要是指使用不同类型指令集和体系架构的计算单元组成系统的计算方式。常见的计算单元类别包括CPU、GPU等协处理器、DSP、ASIC、FPGA等。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">异构计算近年来得到更多关注，主要是因为通过提升CPU时钟频率和内核数量而提高计算能力的传统方式遇到了散热和能耗瓶颈。而与此同时，GPU等专用计算单元虽然工作频率较低，具有更多的内核数和并行计算能力，总体性能-芯片面积比和性能-功耗比都很高，却远远没有得到充分利用。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">广义上，不同计算平台的各个层次上都存在异构现象，除硬件层的指令集、互联方式、内存层次之外，软件层中应用二进制接口、API、语言特性底层实现等的不同，对于上层应用和服务而言，都是异构的。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">从实现的角度来说，异构计算就是制定出一系列的软件与硬件的标准，让不同类型的计算设备能够共享计算的过程和结果。同时不断优化和加速计算的过程，使其具备更高的计算效能。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">计算的发展历程：从32bit到异构计算</span></strong></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑"><strong>2003年以前，是32bit的时代。</strong>处理器制造厂商，不断提升制造工艺技术，使用更精细的制程来制造处理器。同时也不断提高处理器的时脉，如133MHz、166MHz、200MHz、300MHz&hellip;&hellip;最终频率提升到了3GHz后，就难作寸进了。到目前为止我们也未曾见到Intel和AMD发布高于4GHz主频的处理器产品。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">2003年出现了x86-64</span></strong><span style="font-family: 微软雅黑">，有时会简称为&ldquo;x64&rdquo;，是64位微处理器架构及其相应指令集的一种，也是Intel x86架构的延伸产品。&ldquo;x86-64&rdquo;1999由AMD设计，AMD首次公开64位集以扩充给IA-32，称为x86-64（后来改名为AMD64）。其后也为英特尔所采用，现时英特尔称之为&ldquo;Intel 64&rdquo;，在之前曾使用过Clackamas Technology (CT)、IA-32e及EM64T。外界多使用&quot;x86-64&quot;或&quot;x64&quot;去称呼此64位架构，从而保持中立，不偏袒任何厂商。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">AMD64代表AMD放弃了跟随Intel标准的一贯作风，选择了像把16位的Intel 8086扩充成32位的80386般，去把x86架构扩充成64位版本，且兼容原有标准。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">AMD64架构在IA-32上新增了64位暂存器，并兼容早期的16位和32位软件，可使现有以x86为对象的编译器容易转为AMD64版本。除此之外，NX bit也是引人注目的特色之一。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">不少人认为，像DEC Alpha般的64位RISC芯片，最终会取代现有过时及多变的x86架构。但事实上，为x86系统而设的应用软件实在太庞大，成为Alpha不能取代x86的主要原因，AMD64能有效地把x86架构移至64位的环境，并且能兼容原有的x86应用程序。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">2006年出现了双核心多核心。</span></strong><span style="font-family: 微软雅黑">多核心，也叫多微处理器核心是将两个或更多的独立处理器封装在一起的方案，通常在一个集成电路（IC）中。双核心设备只有两个独立的微处理器。一般说来，多核心微处理器允许一个计算设备在不需要将多核心包括在独立物理封装时执行某些形式的线程级并发处理（Thread-Level Parallelism，TLP）这种形式的TLP通常被认为是芯片级多处理。在游戏中你必须要使用驱动程序来利用第二颗核心。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">此后处理器制造厂商发现，利用多核心架构可以在不提升处理器频率的情况下，继续不断提升处理器的效能。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">2008年通用计算GPGPU。</span></strong><span style="font-family: 微软雅黑">通用图形处理器（General-purpose computing on graphics processing units，简称GPGPU），是一种利用处理图形任务的图形处理器来计算原本由中央处理器处理的通用计算任务。这些通用计算常常与图形处理没有任何关系。由于现代图形处理器强大的并行处理能力和可编程流水线，令流处理器可以处理非图形数据。特别在面对单指令流多数据流（SIMD），且数据处理的运算量远大于数据调度和传输的需要时，通用图形处理器在性能上大大超越了传统的中央处理器应用程序。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">3D显示卡的性能从NVIDIA的GeForce256时代就颇受瞩目，时间到了2008年，显示卡的计算能力开始被用在实际的计算当中。并且其处理的速度也远远超越了传统的x86处理器。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">2010年CPU+GPU异构计算。</span></strong><span style="font-family: 微软雅黑">对于GPGPU表现出的惊人计算能力叫人为之折服，但是在显卡进行计算的同时，处理器处于闲置状态。由此处理器厂商也想参与到计算中来，他们希望CPU和GPU能够协同运算，完成那些对计算量有着苛刻要求的应用。同时也希望将计算机的处理能力再推上一个新的高峰。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">天河星云：异构计算大显神威</span></strong></p><p style="text-align: center; padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">2009年，国际TOP500组织TOP500.org在网站上公布了全球超级计算机TOP500强排行榜，由国防科学技术大学研制，部署在国家超级计算天津中心，中国千万亿次超级计算机&ldquo;天河一号&rdquo;位居第一位，实测运算速度可以达到每秒2570万亿次。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">&ldquo;天河一号&rdquo;耗资6亿元，连接了上万个美国英特尔和Nvidia公司制造的CPU和GPU，属异构混合架构。天河一号的配置是14336颗英特尔六核至强X5670 2.93GHz CPU和7168颗Nvidia Tesla M2050 GPU和2048颗自主研发的八核飞腾FT-1000 CPU。处理内核数突破20万颗，是2008年24576颗的8.25倍。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">排名第三的是曙光公司研制的&ldquo;星云&rdquo;高性能计算机，其实测运算速度达到每秒1270万亿次。petaflop/s，千万亿次计算单位。星云系统峰值为每秒3000万亿次（3PFlops），实测Linpack值每秒1271万亿次（1.271PFlops），是中国第一台、世界第三台实测双精度浮点计算超千万亿次的超级计算机。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">星云超级计算机采用自主设计的HPP体系结构，处理器是32nm工艺的六核至强X5650，并且采用了Nvidia Tesla C2050 GPU做协处理，由4640个计算单元组成。它采用了高效异构协同计算技术，系统包括了9280颗通用CPU和4640颗专用GPGPU组成。计算网络采用了单向40Gbps QDR Infiniband技术，核心存储采用了自主设计的Parastor高速I/O系统。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">迥异：不同计算架构的特点</span></strong></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">上文提到的采用的异构计算架构都属于大型计算机的范畴。对于个人计算机而言，尤其是x86架构的计算机，异构计算的步伐则要慢许多。这是因为，无论是处理器还是显示卡，又或者其他运算部件，都有其自身的架构和特性。他们是针对不同领域，面向不同应用所设计的芯片。所以他们在功能性方面千差万别。要想将他们都统一起来，除了需要制定共同的规范和标准之外，还要针对其计算的特点设计软件。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">举例来说，CPU和GPU在进行计算时，就有许多不同。对于处理器来说，它是一颗通用处理器。它要应对各种类型的计算应用。无论是数学方面的，还是逻辑方面的运算。我们可以看到，一颗比较常规的处理器其中的ALU计算单元仅仅占据整个核心面积的25%以内。在处理器中，超过50%的核心面积用来制作Cache高速缓存，无论是L1、L2还是片上的L3。而另外还有25%的核心面积用来作为控制器。它控制着处理管线的运作，控制着各种分支预测，让多核心处理器可以更有效率。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">而我们再反观GPU，其结构要简单的多。GPU的任务是加速3D像素的计算。因此我们在显卡中可以看到数以百计的流处理器单元或者是CUDA核心。而在整个计算过程中，GPU承担的逻辑计算任务非常小。同时它有着更宽的显存带宽，有着更高速的显存。所以在GPU芯片中，也就无需更大容量的片上缓存机制。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">通过上文的分析，我们可以看到CPU的在处理时，适合作所有工作，各个方面都比较平均。逻辑处理能力要比GPU快，但是对于数学计算方面，其速度不如具有海量处理核心的GPU快。而GPU方面，数学计算性能强大，大规模并行处理机制强大，但是逻辑处理能力不足，仅仅能在某些计算领域应用。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">异构计算芯片</span></strong></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">AMD 2011年发布的新一代A8处理器，就是一颗真正意义上的异构计算处理器。AMD平台在CPU表现一直比较薄弱，如何把AMD显卡方面的优势均衡到CPU方面，AMD一直为这个目标而努力，而在AMD APU上，AMD已经初步实现了这个目的。这就是APU异构计算技术。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">CPU的设计让其比较擅长于处理不规则数据结构和不可预测的存取模式，以及递归算法、分支密集型代码和单线程程序。这类程序任务拥有复杂的指令调度、循环、分支、逻辑判断以及执行等步骤。而GPU擅于处理规则数据结构和可预测存取模式。而APU的设计理念则正是让CPU和GPU完美合作，集合两者的长处，用异构计算来达到整体性能的最佳化。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">这样的异构计算芯片可以充分发挥不同计算部件的优势。当需要进行较多逻辑计算时，可以使用CPU部分完成。当需要大量的浮点运算时，可以借用GPU的浮点运算处理管线来完成。同时如果处理器的某些核心正处于空闲，也可以让其加入到计算中来。由此可见异构计算不仅仅是需要统一起不同类型的计算部件，同时也需要有针对性的让更适合的硬件作适用的计算工作。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">新的计算架构需要全新的软件标准</span></strong></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">对于异构计算来说，更重要的软件。虽然现在我们看到许多计算机中都应用了GPGPU的通用计算，使用显卡来进行大规模的并行计算任务，但是在这个过程中，处理器就被闲置了。例如许多转码程序在运行的时候，仅仅是显卡在跑，而处理器并未参与到转码加速中来。将异构的运算部件，全部有效的调用起来，这是一件困难的编程工作。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">OpenCL：无人能模仿 很难被超越</span></strong></p><p style="text-align: center; padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><img class="insertimage" src="attachment.php?fid=1483" border="0" width="450" height="318" /></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">2008年6月的WWDC大会上，苹果提出了OpenCL规范，旨在提供一个通用的开放API，在此基础上开发GPU通用计算软件。随后，Khronos Group宣布成立GPU通用计算开放行业标准工作组，以苹果的提案为基础创立OpenCL行业规范。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">OpenCL (Open Computing Language，开放计算语言) 是一个为异构平台编写程序的框架，此异构平台可由CPU，GPU或其他类型的处理器组成。OpenCL由一门用于编写kernels（在OpenCL设备上运行的函数）的语言（基于C99）和一组用于定义并控制平台的API组成。OpenCL提供了基于任务分区和数据分区的并行计算机制。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">OpenCL类似于另外两个开放的工业标准OpenGL和OpenAL，这两个标准分别用于三维图形和计算机音频方面。OpenCL扩展了GPU用于图形生成之外的能力。OpenCL由非盈利性技术组织Khronos Group掌管。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">OpenCL最初苹果公司开发，拥有其商标权，并在与AMD，IBM，英特尔和nVIDIA技术团队的合作之下初步完善。随后，苹果将这一草案提交至Khronos Group。2011年11月15日，OpenCL 1.2发布。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">虽然苹果制定OpenCL的私心路人皆知，希望通过OpenGL来让自家的Mac电脑可以顺利的使用两个显卡巨头的产品做GPGPU运算。但是苹果的这一举措却为未来的x86平台异构计算奠定了坚实的基础。因为无论是CUDA还是FireStream，无论是CUDA核心还是流处理器，软件开发人员都可以通过OpenCL来支持。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">C++ AMP：微软发布异构计算编程语言</span></strong></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">Microsoft DirectCompute是一个应用程序接口（API），允许Windows Vista或Windows 7平台上运行的程序利用图形处理器（GPU）进行通用计算，DirectCompute是Microsoft DirectX的一部分。虽然DirectCompute最初在DirectX 11 API中得以实现，但支持DX10的GPU可以利用此API的一个子集进行通用计算，支持DX11的GPU则可以使用完整的DirectCompute功能。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">相比OpenGL丰富的功能和体系化的SDK来说，DirectCompute仅仅是以一个简单的API存于世上，显然不能赢得更多厂商的关注。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">自从AMD发布了Llano处理器，异构计算就真正进入了寻常百姓的家中。虽然OpenCL作为通用大规模并行计算的行业领军标准，得到了AMD、Intel、NVIDIA等芯片业巨头和大量行业厂商的支持，但唯独缺少了微软。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">在2011年AMD Fusion（AFDS前身）开发者峰会上，微软终于拿出了自己的异构计算编程语言：&ldquo;C++ AMP&rdquo;，其中AMP三个字母是&ldquo;accelerated massive parallelism&rdquo;的缩写，也就是加速大规模并行的意思。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">C++ AMP是微软Visual Studio和C++编程语言的新扩展包，用于辅助开发人员充分适应现在和未来的高度并行和异构计算环境。它使用C++语言的句法，将捆绑在下个版本的Visual Studio中发布，预计会在今年晚些时候放出测试版本。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">为了与OpenCL相抗衡，微软宣布C++ AMP标准将是一种开放的规范，允许其它编译器集成和支持。这无疑是对OpenCL的最直接挑战。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">Java Aparapi</span></strong></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">Aparapi允许Java开发人员发挥GPU和APU设备的计算能力，通过在GPU上执行并行代码，可以获得远远超过只是用本地CPU计算的能力。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">Java代码在运行时，Aparapi会先将字节码转换为OpenCL在GPU上执行。如果由于某种原因Aparapi不能在GPU上执行，它将会在Java线程池中运行。</span></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><strong><span style="font-family: 微软雅黑">一触即发：异构计算行业标准大战</span></strong></p><p style="padding-bottom: 0px; list-style-type: none; margin-top: 0px; margin-bottom: 1.5em; text-decoration: none; padding-top: 0px"><span style="font-family: 微软雅黑">OpenCL由多家主力厂商支持，技术纷争不断，为了自家产品的利益难免在新版本制定方面出现歧路。而微软的C++ AMP会与Windows紧密贴合起来。虽然也属于开放性的标准，但是微软对其未来方向的掌控，其执行应该更有效力。战国纷争，异构计算行业标准大战一触即发，不止异构计算未来鹿死谁手？</span></p></div></div>
]]>
</description>
</item>
</channel>
</rss>